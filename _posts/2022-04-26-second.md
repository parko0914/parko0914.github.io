```python
#êµ¬ê¸€ë“œë¼ì´ë¸Œ ì—°ë™
from google.colab import drive
drive.mount('/content/drive')
```

    Mounted at /content/drive
    


```python
# gpu ì¼œê¸°
import torch
device = torch.device("cuda:0")
```


```python
# ì €ì¥ ê²½ë¡œ ë¯¸ë¦¬ ì§€ì •
path = '/content/drive/MyDrive/nlp_c/'
```

# **ë¡œê·¸íŒŒì¼ ì—°ë™ ì‹œí‚¤ê¸°**


```python
import logging

def make_logger(name=None):
    #1 logger instanceë¥¼ ë§Œë“ ë‹¤.
    logger = logging.getLogger(name)

    #2 loggerì˜ levelì„ ê°€ì¥ ë‚®ì€ ìˆ˜ì¤€ì¸ DEBUGë¡œ ì„¤ì •í•´ë‘”ë‹¤.
    logger.setLevel(logging.DEBUG)

    #3 formatter ì§€ì •
    formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    
    #4 handler instance ìƒì„±
    console = logging.StreamHandler()
    file_handler = logging.FileHandler(filename=path + "logs/correct_final.log",
                                       encoding = 'utf-8')
    
    #5 handler ë³„ë¡œ ë‹¤ë¥¸ level ì„¤ì •
    console.setLevel(logging.INFO)
    file_handler.setLevel(logging.DEBUG)

    #6 handler ì¶œë ¥ format ì§€ì •
    console.setFormatter(formatter)
    file_handler.setFormatter(formatter)

    #7 loggerì— handler ì¶”ê°€
    logger.addHandler(console)
    logger.addHandler(file_handler)

    return logger
```


```python
logger = make_logger()

logger.debug("debug logging")
logger.info("info logging")
logger.warning("warning logging")
logger.error("error logging")
logger.critical("critical logging")
```

    2022-04-12 17:21:52,192 - root - INFO - info logging
    2022-04-12 17:21:52,194 - root - WARNING - warning logging
    2022-04-12 17:21:52,195 - root - ERROR - error logging
    2022-04-12 17:21:52,198 - root - CRITICAL - critical logging
    

# **í•„ìš”í•œ í™˜ê²½ ë‹¤ìš´ ë° êµ¬ì¶•**

## í•™ìŠµëª¨ë¸ íŒ¨í‚¤ì§€ ë‹¤ìš´ ë° êµ¬ì¶•(KoBERT)


```python
#ê¹ƒí—ˆë¸Œì—ì„œ KoBERT íŒŒì¼ ë¡œë“œ
!pip install ipywidgets  # for vscode
!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master
```

    Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (7.7.0)
    Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (3.6.0)
    Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.2.0)
    Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.5.0)
    Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (1.1.0)
    Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (0.2.0)
    Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (4.10.1)
    Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.1.1)
    Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.1.1)
    Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.5)
    Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)
    Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (1.0.18)
    Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)
    Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (2.6.1)
    Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (0.8.1)
    Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)
    Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (57.4.0)
    Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets) (4.9.2)
    Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets) (4.3.3)
    Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.10.0.2)
    Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (4.11.3)
    Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.1)
    Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (5.4.0)
    Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.4.0)
    Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.7.0)
    Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets) (0.2.5)
    Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets) (1.15.0)
    Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (5.3.1)
    Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)
    Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.11.3)
    Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.13.3)
    Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.6.1)
    Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (22.3.0)
    Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)
    Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.0)
    Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0.1)
    Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)
    Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.6.0)
    Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.1.0)
    Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)
    Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)
    Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.4)
    Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.3)
    Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)
    Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.7)
    Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master
      Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-t4wud7uv
      Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-t4wud7uv
    Collecting boto3
      Downloading boto3-1.21.38-py3-none-any.whl (132 kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 132 kB 4.1 MB/s 
    [?25hCollecting gluonnlp>=0.6.0
      Downloading gluonnlp-0.10.0.tar.gz (344 kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 344 kB 33.4 MB/s 
    [?25hCollecting mxnet>=1.4.0
      Downloading mxnet-1.9.0-py3-none-manylinux2014_x86_64.whl (47.3 MB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47.3 MB 1.1 MB/s 
    [?25hCollecting onnxruntime==1.8.0
      Downloading onnxruntime-1.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.5 MB 79.4 MB/s 
    [?25hCollecting sentencepiece>=0.1.6
      Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2 MB 59.6 MB/s 
    [?25hRequirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (1.10.0+cu111)
    Collecting transformers>=4.8.1
      Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.0 MB 49.4 MB/s 
    [?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (3.17.3)
    Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (1.21.5)
    Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (2.0)
    Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (0.29.28)
    Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (21.3)
    Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet>=1.4.0->kobert==0.2.3) (2.23.0)
    Collecting graphviz<0.9.0,>=0.8.1
      Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)
    Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (1.24.3)
    Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (3.0.4)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (2021.10.8)
    Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (2.10)
    Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->kobert==0.2.3) (3.10.0.2)
    Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (3.6.0)
    Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (2019.12.20)
    Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (4.63.0)
    Collecting huggingface-hub<1.0,>=0.1.0
      Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77 kB 8.6 MB/s 
    [?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (4.11.3)
    Collecting tokenizers!=0.11.3,<0.13,>=0.11.1
      Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.5 MB 56.7 MB/s 
    [?25hCollecting sacremoses
      Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 895 kB 81.6 MB/s 
    [?25hCollecting pyyaml>=5.1
      Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596 kB 79.5 MB/s 
    [?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp>=0.6.0->kobert==0.2.3) (3.0.7)
    Collecting jmespath<2.0.0,>=0.7.1
      Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)
    Collecting botocore<1.25.0,>=1.24.38
      Downloading botocore-1.24.38-py3-none-any.whl (8.7 MB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.7 MB 51.5 MB/s 
    [?25hCollecting s3transfer<0.6.0,>=0.5.0
      Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79 kB 11.4 MB/s 
    [?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1
      Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127 kB 91.6 MB/s 
    [?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.38->boto3->kobert==0.2.3) (2.8.2)
    Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.38->boto3->kobert==0.2.3) (1.15.0)
    Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.8.1->kobert==0.2.3) (3.7.0)
    Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.2.3) (7.1.2)
    Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.2.3) (1.1.0)
    Building wheels for collected packages: kobert, gluonnlp
      Building wheel for kobert (setup.py) ... [?25l[?25hdone
      Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15674 sha256=234c7d6793d34f714a8fb2c75ec7e2d7872a43a5a57b8babbcdaff80a8072180
      Stored in directory: /tmp/pip-ephem-wheel-cache-lwdz75rt/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0
      Building wheel for gluonnlp (setup.py) ... [?25l[?25hdone
      Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595739 sha256=063abdebf78d0703cc31721a8960e0949f2618e2401a0934306b55de35a73a33
      Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00
    Successfully built kobert gluonnlp
    Installing collected packages: urllib3, jmespath, pyyaml, botocore, tokenizers, sacremoses, s3transfer, huggingface-hub, graphviz, transformers, sentencepiece, onnxruntime, mxnet, gluonnlp, boto3, kobert
      Attempting uninstall: urllib3
        Found existing installation: urllib3 1.24.3
        Uninstalling urllib3-1.24.3:
          Successfully uninstalled urllib3-1.24.3
      Attempting uninstall: pyyaml
        Found existing installation: PyYAML 3.13
        Uninstalling PyYAML-3.13:
          Successfully uninstalled PyYAML-3.13
      Attempting uninstall: graphviz
        Found existing installation: graphviz 0.10.1
        Uninstalling graphviz-0.10.1:
          Successfully uninstalled graphviz-0.10.1
    [31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
    datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.[0m
    Successfully installed boto3-1.21.38 botocore-1.24.38 gluonnlp-0.10.0 graphviz-0.8.4 huggingface-hub-0.5.1 jmespath-1.0.0 kobert-0.2.3 mxnet-1.9.0 onnxruntime-1.8.0 pyyaml-6.0 s3transfer-0.5.2 sacremoses-0.0.49 sentencepiece-0.1.96 tokenizers-0.11.6 transformers-4.18.0 urllib3-1.25.11
    


```python
# í•„ìš”í•œ ëª¨ë“ˆ ë¡œë”©
import pandas as pd
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gluonnlp as nlp
import numpy as np
from tqdm.notebook import tqdm
from tqdm import tqdm_notebook
```


```python
#kobert
from kobert.utils import get_tokenizer
from kobert.pytorch_kobert import get_pytorch_kobert_model

#transformers
from transformers import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup
```


```python
#BERT ëª¨ë¸, Vocabulary ë¶ˆëŸ¬ì˜¤ê¸°
bertmodel, vocab = get_pytorch_kobert_model()
```

    /content/.cache/kobert_v1.zip[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]
    /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]
    


```python

```

# **ë§ì¶¤ë²• ê²€ì‚¬í•œ ë°ì´í„° ë¶ˆëŸ¬ì˜¨ ë’¤ ì „ì²˜ë¦¬(EDA ë°©ë²• í¬í•¨)**


```python
# ë§ì¶¤ë²• ê²€ì‚¬ ì™„ë£Œí•œ train íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
train_d = pd.read_csv(path + 'final_data/' + 'correct_train_fin.csv', encoding = 'utf-8-sig')
print(len(train_d))
```

    705530
    


```python
# index ë³€ê²½ -> ì•„ë˜ ëª¨ë¸ ìƒì„±ì—ì„œ index ì˜¤ë¥˜ ë°œìƒí•˜ì—¬ 0ë¶€í„° re-indexing
# predict ê°’ ì¶”ì¶œí•˜ê³ , ë‹¤ì‹œ ì›ë˜ y ê°’ìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ í•´ì•¼í•  ë“¯
# ì†Œë¶„ë¥˜ dictionary
y_dict = pd.DataFrame({'origin_y' : train_d['y'].unique()}).sort_values(by = 'origin_y')
y_dict['y'] = np.arange(0, len(y_dict))
y_dict = y_dict.astype('str')
s_dict0 = y_dict.set_index('origin_y').to_dict()['y'] # ì²˜ìŒ yê°’ì„ ëª¨ë¸ train ì„ ìœ„í•´ re-indexing
s_dict1 = y_dict.set_index('y').to_dict()['origin_y'] # ë’¤ì— ì˜ˆì¸¡ê°’ ë‹¤ì‹œ y ê°’ìœ¼ë¡œ return í•  ë•Œ ì‚¬ìš©
```


```python
# kobert ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ reindexing í•œ dictionary ì €ì¥ -> í›„ì— ëª¨ë¸ ì˜ˆì¸¡ê°’ ë„ì¶œ í›„, ê¸°ì¡´ yê°’ìœ¼ë¡œ ë˜ëŒë¦¬ê¸° ìœ„í•¨
import pickle
with open(path+ 'final_data/' + 's_dictionary', 'wb') as f:
    pickle.dump(s_dict1, f)
```


```python
train_d['y_s'] = train_d['y'].astype('str')
train_d['label_s'] = train_d['y_s'].map(s_dict0)
```

## EDA ë¶€ë¶„


```python
# text augmentation
# pip install -U nltk
import nltk; 
nltk.download('omw-1.4');
# nltk.download('wordnet') # ì˜ë¬¸ ë²„ì „
```

    [nltk_data] Downloading package omw-1.4 to /root/nltk_data...
    [nltk_data]   Unzipping corpora/omw-1.4.zip.
    


```python
# eda í´ë” ìƒì„±
% cd /content/drive/MyDrive/nlp_c
# !git clone https://github.com/jasonwei20/eda_nlp
# !git clone https://github.com/catSirup/KorEDA
# edaëŠ” eda_nlp/code í´ë”ì—, wordnet.pickle ì€ eda_nlp í´ë”ë¡œ ì´ë™ì‹œí‚¤ê³ , ì§„í–‰
% cd eda_nlp/
# ì¶”ê°€ì ìœ¼ë¡œ augment.py 64ë²ˆì§¸ í•­ì— eda -> EDAë¡œ ë³€ê²½í•´ì•¼ ì‹¤í–‰ë¨
```

    /content/drive/MyDrive/nlp_c
    /content/drive/MyDrive/nlp_c/eda_nlp
    


```python
s_class_n = pd.DataFrame(train_d['y_s'].value_counts().sort_values())
# s_class_n.to_csv(path + 'testset_class.csv', index=False, encoding='EUC-KR')
s_class = s_class_n[s_class_n['y_s'] < 500].index.tolist()
len(s_class)
```




    85




```python
# nìˆ˜ê°€ ë¶€ì¡±í•œ class aug_num ì°¨ë“±ìœ¼ë¡œ ì¦ê°•(ì ì€ ìˆœì„œëŒ€ë¡œ 20, 10 ,5) -> ìƒëŒ€ì ìœ¼ë¡œ ë¶€ì¡±í•œ í´ë˜ìŠ¤ ë°ì´í„° ë¹„ìœ¨ì´ ë” ë†’ì•„ì§€ëŠ” ê²ƒì„ ì¡°ê¸ˆì´ë¼ë„ ë°©ì§€í•˜ê³ ì í•¨
s_class1 = s_class[:30]
s_class2 = s_class[30:60]
s_class3 = s_class[60:]
```


```python
few_d1 = train_d[train_d['y_s'].isin(s_class1)]
few_d2 = train_d[train_d['y_s'].isin(s_class2)]
few_d3 = train_d[train_d['y_s'].isin(s_class3)]
```


```python
# nì´ 100ê°œ ì´í•˜ì¸ í´ë˜ìŠ¤ ë½‘ì•„ì„œ augmentation ê°€ëŠ¥í•œ íŒŒì¼ í˜•íƒœë¡œ ë§Œë“¤ì–´ì£¼ê¸°
txt_aug_list = [str(a) + '\t' + str(b) for a, b in zip(few_d1['label_s'], few_d1['clean_done'])]
with open(path + 'final_data/' + 'text_aug_1.txt', 'w') as f:
  f.write('\n'.join(txt_aug_list) + '\n')

txt_aug_list = [str(a) + '\t' + str(b) for a, b in zip(few_d2['label_s'], few_d2['clean_done'])]
with open(path + 'final_data/' + 'text_aug_2.txt', 'w') as f:
  f.write('\n'.join(txt_aug_list) + '\n')

txt_aug_list = [str(a) + '\t' + str(b) for a, b in zip(few_d3['label_s'], few_d3['clean_done'])]
with open(path + 'final_data/' + 'text_aug_3.txt', 'w') as f:
  f.write('\n'.join(txt_aug_list) + '\n')

# input file í˜•ì‹ -> txt íŒŒì¼ ë‚´ í•œ í–‰ ë‹¹ label + \t + text í˜•íƒœë¡œ ë“¤ì–´ê°„ íŒŒì¼ 
# SR: Synonym Replacement, íŠ¹ì • ë‹¨ì–´ë¥¼ ìœ ì˜ì–´ë¡œ êµì²´
# RI: Random Insertion, ì„ì˜ì˜ ë‹¨ì–´ë¥¼ ì‚½ì…
# RS: Random Swap, ë¬¸ì¥ ë‚´ ì„ì˜ì˜ ë‘ ë‹¨ì–´ì˜ ìœ„ì¹˜ë¥¼ ë°”ê¿ˆ
# RD: Random Deletion: ì„ì˜ì˜ ë‹¨ì–´ë¥¼ ì‚­ì œ
!python code/augment.py --input=/content/drive/MyDrive/nlp_c/final_data/text_aug_1.txt --output=/content/drive/MyDrive/nlp_c/final_data/test_aug_eda_1.txt --num_aug=20 --alpha_sr=0.1 --alpha_rd=0.2 --alpha_ri=0.1 --alpha_rs=0.0
!python code/augment.py --input=/content/drive/MyDrive/nlp_c/final_data/text_aug_2.txt --output=/content/drive/MyDrive/nlp_c/final_data/test_aug_eda_2.txt --num_aug=10 --alpha_sr=0.1 --alpha_rd=0.2 --alpha_ri=0.1 --alpha_rs=0.0
!python code/augment.py --input=/content/drive/MyDrive/nlp_c/final_data/text_aug_3.txt --output=/content/drive/MyDrive/nlp_c/final_data/test_aug_eda_3.txt --num_aug=5 --alpha_sr=0.1 --alpha_rd=0.2 --alpha_ri=0.1 --alpha_rs=0.0
```

    generated augmented sentences with eda for /content/drive/MyDrive/nlp_c/final_data/text_aug_1.txt to /content/drive/MyDrive/nlp_c/final_data/test_aug_eda_1.txt with num_aug=20
    generated augmented sentences with eda for /content/drive/MyDrive/nlp_c/final_data/text_aug_2.txt to /content/drive/MyDrive/nlp_c/final_data/test_aug_eda_2.txt with num_aug=10
    generated augmented sentences with eda for /content/drive/MyDrive/nlp_c/final_data/text_aug_3.txt to /content/drive/MyDrive/nlp_c/final_data/test_aug_eda_3.txt with num_aug=5
    


```python
# augmentation ì™„ë£Œí•œ ë°ì´í„° ë¶ˆëŸ¬ì™€ì„œ ê¸°ì¡´ë°ì´í„°ì…‹ì— ë¶™ì—¬ì£¼ê¸°(augmentation ëŒ€ìƒ ë°ì´í„°ëŠ” ì‚­ì œ)
with open('/content/drive/MyDrive/nlp_c/final_data/test_aug_eda_1.txt', "r") as file:
  strings = file.readlines()
aug_d1 = pd.DataFrame([x.split('\n')[0].split('\t') for x in strings])
aug_d1.columns = ['label_s', 'clean_done']

with open('/content/drive/MyDrive/nlp_c/final_data/test_aug_eda_2.txt', "r") as file:
  strings = file.readlines()
aug_d2 = pd.DataFrame([x.split('\n')[0].split('\t') for x in strings])
aug_d2.columns = ['label_s', 'clean_done']

with open('/content/drive/MyDrive/nlp_c/final_data/test_aug_eda_3.txt', "r") as file:
  strings = file.readlines()
aug_d3 = pd.DataFrame([x.split('\n')[0].split('\t') for x in strings])
aug_d3.columns = ['label_s', 'clean_done']

train_d = train_d[train_d['y_s'].isin(s_class)==False]
sample_d = pd.concat([train_d[['label_s', 'clean_done']], aug_d1, aug_d2, aug_d3], axis = 0, ignore_index = True)
```


```python
sample_d['len'] = sample_d['clean_done'].astype(str).apply(len)
```


```python
i = 798000
sample_d[i:i+50]
```





  <div id="df-716e8000-ebb4-4c61-98a6-94ddd5f329e7">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label_s</th>
      <th>clean_done</th>
      <th>len</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>798000</th>
      <td>139</td>
      <td>ë‹¹ì§„í™”ë ¥ ì˜ˆì¸ì„  í˜„ì¥ì—ì„œ ë‚´ ê¸°íƒ€ í•´ìƒ ìš´ìˆ˜ì—…</td>
      <td>25</td>
    </tr>
    <tr>
      <th>798001</th>
      <td>139</td>
      <td>ë‹¹ì§„í™”ë ¥ ë‚´ í˜„ì¥ì—ì„œ ì˜ˆì¸ì„  ê¸°íƒ€ í•´ìƒ ìš´ìˆ˜ì—…</td>
      <td>25</td>
    </tr>
    <tr>
      <th>798002</th>
      <td>103</td>
      <td>ê°ì¢… ê±´ì„¤í˜„ì¥ íê¸°ë¬¼ì„ ì¤‘ê°„ì²˜ë¦¬</td>
      <td>17</td>
    </tr>
    <tr>
      <th>798003</th>
      <td>103</td>
      <td>ê°ì¢… ê±´ì„¤í˜„ì¥ íê¸°ë¬¼ì„ ì¤‘ê°„ì²˜ë¦¬</td>
      <td>17</td>
    </tr>
    <tr>
      <th>798004</th>
      <td>103</td>
      <td>ê°ì¢… ê±´ì„¤í˜„ì¥ íê¸°ë¬¼ì„ ì¤‘ê°„ì²˜ë¦¬</td>
      <td>17</td>
    </tr>
    <tr>
      <th>798005</th>
      <td>103</td>
      <td>ê°ì¢… ê±´ì„¤í˜„ì¥ íê¸°ë¬¼ì„</td>
      <td>12</td>
    </tr>
    <tr>
      <th>798006</th>
      <td>103</td>
      <td>ê±´ì„¤í˜„ì¥ ê°ì¢… íê¸°ë¬¼ì„ ì¤‘ê°„ì²˜ë¦¬</td>
      <td>17</td>
    </tr>
    <tr>
      <th>798007</th>
      <td>103</td>
      <td>ê°ì¢… ê±´ì„¤í˜„ì¥ íê¸°ë¬¼ì„ ì¤‘ê°„ì²˜ë¦¬</td>
      <td>17</td>
    </tr>
    <tr>
      <th>798008</th>
      <td>196</td>
      <td>í™˜ê²½ ê´€ë¦¬ì‚¬ì—…ì†Œì—ì„œ í•˜ìˆ˜ ë° ìˆ˜í–‰í•˜ì—¬ íê¸°ë¬¼ í™˜ê²½ë³´ì¡´ í–‰ì •</td>
      <td>32</td>
    </tr>
    <tr>
      <th>798009</th>
      <td>196</td>
      <td>í™˜ê²½ ê´€ë¦¬ì‚¬ì—…ì†Œì—ì„œ í•˜ìˆ˜ íê¸°ë¬¼ ë° ìœ„ìƒì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ì—¬ íê¸°ë¬¼ í™˜ê²½ë³´ì¡´ ê²½</td>
      <td>41</td>
    </tr>
    <tr>
      <th>798010</th>
      <td>196</td>
      <td>í™˜ê²½ ê´€ë¦¬ì‚¬ì—…ì†Œì—ì„œ í•˜ìˆ˜ íê¸°ë¬¼ ìœ„ìƒì²˜ë¦¬ë¥¼ ë° ìˆ˜í–‰í•˜ì—¬ íê¸°ë¬¼ í™˜ê²½ë³´ì¡´ í–‰ì •</td>
      <td>42</td>
    </tr>
    <tr>
      <th>798011</th>
      <td>196</td>
      <td>í™˜ê²½ í–‰ ê´€ë¦¬ì‚¬ì—…ì†Œì—ì„œ í•˜ìˆ˜ íê¸°ë¬¼ ë° ìœ„ìƒì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ì—¬ íê¸°ë¬¼ í™˜ê²½ë³´ì¡´ í–‰ì •</td>
      <td>44</td>
    </tr>
    <tr>
      <th>798012</th>
      <td>196</td>
      <td>í™˜ê²½ ê´€ë¦¬ì‚¬ì—…ì†Œì—ì„œ í•˜ìˆ˜ íê¸°ë¬¼ ë° ìœ„ìƒì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ì—¬ íê¸°ë¬¼ í–‰ í™˜ê²½ë³´ì¡´ í–‰ì •</td>
      <td>44</td>
    </tr>
    <tr>
      <th>798013</th>
      <td>196</td>
      <td>í™˜ê²½ ê´€ë¦¬ì‚¬ì—…ì†Œì—ì„œ í•˜ìˆ˜ íê¸°ë¬¼ ë° ìœ„ìƒì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ì—¬ íê¸°ë¬¼ í™˜ê²½ë³´ì¡´ í–‰ì •</td>
      <td>42</td>
    </tr>
    <tr>
      <th>798014</th>
      <td>103</td>
      <td>ê±´ì„¤íê¸°ë¬¼ì„ ë°›ì•„ ë¶„ë¦¬ ê±´ì„¤íê¸°ë¬¼ì²˜ë¦¬ ì²˜ë¦¬</td>
      <td>23</td>
    </tr>
    <tr>
      <th>798015</th>
      <td>103</td>
      <td>ê±´ì„¤íê¸°ë¬¼ì„ ë°›ì•„ ë¶„ë¦¬ ì²˜ë¦¬</td>
      <td>15</td>
    </tr>
    <tr>
      <th>798016</th>
      <td>103</td>
      <td>ê±´ì„¤íê¸°ë¬¼ì„ ë¶„ ë°›ì•„ ë¶„ë¦¬ ê±´ì„¤íê¸°ë¬¼ì²˜ë¦¬ ì²˜ë¦¬</td>
      <td>25</td>
    </tr>
    <tr>
      <th>798017</th>
      <td>103</td>
      <td>ê±´ì„¤íê¸°ë¬¼ì„ ë°›ì•„ ë¦¬ ê±´ì„¤íê¸°ë¬¼ì²˜ë¦¬ ì²˜ë¦¬</td>
      <td>22</td>
    </tr>
    <tr>
      <th>798018</th>
      <td>103</td>
      <td>ê±´ì„¤íê¸°ë¬¼ì„ ë°›ì•„ ë¶„ë¦¬ ì²˜ë¦¬ ê±´ì„¤íê¸°ë¬¼ì²˜ë¦¬</td>
      <td>23</td>
    </tr>
    <tr>
      <th>798019</th>
      <td>103</td>
      <td>ê±´ì„¤íê¸°ë¬¼ì„ ë°›ì•„ ë¶„ë¦¬ ê±´ì„¤íê¸°ë¬¼ì²˜ë¦¬ ì²˜ë¦¬</td>
      <td>23</td>
    </tr>
    <tr>
      <th>798020</th>
      <td>83</td>
      <td>ì‚¬ì—…ì¥ì—ì„œ ê¸ˆì†ìœ¼ë¡œ ìë™ì°¨ ë ˆ ì œì¡°</td>
      <td>19</td>
    </tr>
    <tr>
      <th>798021</th>
      <td>83</td>
      <td>ì‚¬ì—…ì¥ì—ì„œ ê¸ˆì†ìœ¼ë¡œ ìë™ì°¨ ëŸ¬ ì œì¡°</td>
      <td>19</td>
    </tr>
    <tr>
      <th>798022</th>
      <td>83</td>
      <td>ì‚¬ì—…ì¥ì—ì„œ ì ê¸ˆì†ìœ¼ë¡œ ìë™ì°¨ íŠ¸ë ˆì¼ëŸ¬ ì œì¡°</td>
      <td>24</td>
    </tr>
    <tr>
      <th>798023</th>
      <td>83</td>
      <td>ì‚¬ì—…ì¥ì—ì„œ ê¸ˆì†ìœ¼ë¡œ ìë™ì°¨ íŠ¸ë ˆì¼ëŸ¬ ì œì¡°</td>
      <td>22</td>
    </tr>
    <tr>
      <th>798024</th>
      <td>83</td>
      <td>íŠ¸ë ˆì¼ëŸ¬ ê¸ˆì†ìœ¼ë¡œ ìë™ì°¨ ì‚¬ì—…ì¥ì—ì„œ ì œì¡°</td>
      <td>22</td>
    </tr>
    <tr>
      <th>798025</th>
      <td>83</td>
      <td>ì‚¬ì—…ì¥ì—ì„œ ê¸ˆì†ìœ¼ë¡œ ìë™ì°¨ íŠ¸ë ˆì¼ëŸ¬ ì œì¡°</td>
      <td>22</td>
    </tr>
    <tr>
      <th>798026</th>
      <td>116</td>
      <td>ì í¬ì—ì„œ ì†Œë¹„ìë¥¼ ëŒ€ìƒìœ¼ë¡œ í†  ì†Œë§¤</td>
      <td>19</td>
    </tr>
    <tr>
      <th>798027</th>
      <td>116</td>
      <td>ì í¬ì—ì„œ ì†Œë¹„ìë¥¼ ì†Œë§¤ ì˜¤í† ë°”ì´ ëŒ€ìƒìœ¼ë¡œ</td>
      <td>22</td>
    </tr>
    <tr>
      <th>798028</th>
      <td>116</td>
      <td>ì í¬ì—ì„œ ì†Œë¹„ìë¥¼ ëŒ€ìƒìœ¼ë¡œ ì˜¤í† ë°”ì´ ì†Œë§¤</td>
      <td>22</td>
    </tr>
    <tr>
      <th>798029</th>
      <td>116</td>
      <td>ì í¬ì—ì„œ ì†Œë¹„ìë¥¼ ì˜¤ ëŒ€ìƒìœ¼ë¡œ ì˜¤í† ë°”ì´ ì†Œë§¤</td>
      <td>24</td>
    </tr>
    <tr>
      <th>798030</th>
      <td>116</td>
      <td>ì í¬ì—ì„œ ëŒ€ìƒìœ¼ë¡œ ì†Œë¹„ìë¥¼ ì˜¤í† ë°”ì´ ì†Œë§¤</td>
      <td>22</td>
    </tr>
    <tr>
      <th>798031</th>
      <td>116</td>
      <td>ì í¬ì—ì„œ ì†Œë¹„ìë¥¼ ëŒ€ìƒìœ¼ë¡œ ì˜¤í† ë°”ì´ ì†Œë§¤</td>
      <td>22</td>
    </tr>
    <tr>
      <th>798032</th>
      <td>44</td>
      <td>ìš”ì†Œ í™©ì‚° ë°˜ì‘ ê±´ ìŠµì‹</td>
      <td>13</td>
    </tr>
    <tr>
      <th>798033</th>
      <td>44</td>
      <td>ìš”ì†Œ ê±´ í™©ì‚° ë°˜ì‘ ê±´ì¡° ìŠµì‹</td>
      <td>16</td>
    </tr>
    <tr>
      <th>798034</th>
      <td>44</td>
      <td>í™©ì‚° ë°˜ì‘ ê±´ì¡° ìŠµì‹</td>
      <td>11</td>
    </tr>
    <tr>
      <th>798035</th>
      <td>44</td>
      <td>ìš”ì†Œ ê±´ì¡° ë°˜ì‘ í™©ì‚° ìŠµì‹</td>
      <td>14</td>
    </tr>
    <tr>
      <th>798036</th>
      <td>44</td>
      <td>í™©ì‚° ìš”ì†Œ ë°˜ì‘ ê±´ì¡° ìŠµì‹</td>
      <td>14</td>
    </tr>
    <tr>
      <th>798037</th>
      <td>44</td>
      <td>ìš”ì†Œ  í™©ì‚° ë°˜ì‘  ê±´ì¡° ìŠµì‹</td>
      <td>18</td>
    </tr>
    <tr>
      <th>798038</th>
      <td>23</td>
      <td>Nylon Spandex ì—°ì‚¬ ì‹¤ê¼¬ì„ ìŠ¤íŒë±ìŠ¤ê°€</td>
      <td>26</td>
    </tr>
    <tr>
      <th>798039</th>
      <td>23</td>
      <td>Nylon ì—°ì‚¬ Spandex ì‹¤ê¼¬ì„ ìŠ¤íŒë±ìŠ¤ê°€</td>
      <td>26</td>
    </tr>
    <tr>
      <th>798040</th>
      <td>23</td>
      <td>Nylon Spandex ì‹¤ê¼¬ì„ ìŠ¤íŒë±ìŠ¤ê°€</td>
      <td>23</td>
    </tr>
    <tr>
      <th>798041</th>
      <td>23</td>
      <td>Nylon Spandex ì—°ì‚¬ ì‹¤ê¼¬ì„ ìŠ¤íŒë±ìŠ¤ê°€</td>
      <td>26</td>
    </tr>
    <tr>
      <th>798042</th>
      <td>23</td>
      <td>Nylon Spandex ìŠ¤íŒë±ìŠ¤ê°€</td>
      <td>19</td>
    </tr>
    <tr>
      <th>798043</th>
      <td>23</td>
      <td>Nylon Spandex  ì—°ì‚¬ ì‹¤ê¼¬ì„  ìŠ¤íŒë±ìŠ¤ê°€</td>
      <td>28</td>
    </tr>
    <tr>
      <th>798044</th>
      <td>46</td>
      <td>ìš”ì†Œ í™©ì‚°ì²  ìœ ì‚°ì²  ì›ë£Œ ì ê³„ëŸ‰</td>
      <td>18</td>
    </tr>
    <tr>
      <th>798045</th>
      <td>46</td>
      <td>ìš”ì†Œ í™©ì‚°ì²  ì›ë£Œ íˆ¬ì… ê³„ëŸ‰</td>
      <td>15</td>
    </tr>
    <tr>
      <th>798046</th>
      <td>46</td>
      <td>ìš”ì†Œ í™©ì‚°ì²  ìœ ì‚°ì²  ì›ë£Œ íˆ¬ì… ì› ê³„ëŸ‰</td>
      <td>21</td>
    </tr>
    <tr>
      <th>798047</th>
      <td>46</td>
      <td>í™©ì‚°ì²  ìœ ì‚°ì²  ì›ë£Œ íˆ¬ì… ê³„ëŸ‰</td>
      <td>16</td>
    </tr>
    <tr>
      <th>798048</th>
      <td>46</td>
      <td>ìš”ì†Œ í™©ì‚°ì²  ìœ ì‚°ì²  íˆ¬ì… ì›ë£Œ ê³„ëŸ‰</td>
      <td>19</td>
    </tr>
    <tr>
      <th>798049</th>
      <td>46</td>
      <td>ìš”ì†Œ  í™©ì‚°ì²  ìœ ì‚°ì²  ì›ë£Œ íˆ¬ì…  ê³„ëŸ‰</td>
      <td>22</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-716e8000-ebb4-4c61-98a6-94ddd5f329e7')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-716e8000-ebb4-4c61-98a6-94ddd5f329e7 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-716e8000-ebb4-4c61-98a6-94ddd5f329e7');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python

```

# **ì²˜ë¦¬í•œ ë°ì´í„° kobert ëª¨ë¸ì— í•™ìŠµ**


```python
# train & test set ë‚˜ëˆ„ê¸°
from sklearn.model_selection import train_test_split
# dataset_train, dataset_test = train_test_split(train_d, test_size=0.2, shuffle=True, random_state=30)
# dataset_test.to_csv('/content/drive/MyDrive/nlp_c/testset.csv', index=False, encoding = 'EUC-KR')

# stratify ë¥¼ targetìœ¼ë¡œ ì§€ì •í•´ ë¹„ìœ¨ì„ ë§ì¶¤ìœ¼ë¡œì¨, ì„±ëŠ¥í–¥ìƒ ê°€ëŠ¥
# but, í˜„ì¬ target ë³€ìˆ˜ class ë¹„ìœ¨ì˜ ë¶ˆê· í˜•ìœ¼ë¡œ ì˜¤ë¥˜ ë°œìƒ(1,2 ê°œì§œë¦¬ class ë‹¤ìˆ˜ ì¡´ì¬)
dataset_train, dataset_test, y_train, y_test = train_test_split(sample_d['clean_done'],
                               sample_d['label_s'], random_state=132, stratify=sample_d['label_s']) 
# ëª¨ë¸ ê²€ì¦ìš© ë¯¸ë¦¬ ë½‘ì•„ë†“ê¸°
dataset_test.to_csv(path + 'final_data/' + 'testset.csv', index=False, encoding = 'utf-8-sig')
y_test.to_csv(path + 'final_data/' + 'testset_y.csv', index=False, encoding = 'utf-8-sig')
```


```python
dataset_train = [[str(a), str(b)] for a, b in zip(dataset_train, y_train)]
dataset_test = [[str(a), str(b)] for a, b in zip(dataset_test, y_test)]
```


```python
print(len(dataset_train))
print(len(dataset_test))
```

    614287
    204763
    


```python
print(dataset_train[:10])
print(dataset_test[:10])
```

    [['ì‚¬ì—…ì¥ì—ì„œ ì˜ë¢°ë¥¼ ë°›ì•„ ì² ê·¼ ë° ì½˜í¬ë¦¬íŠ¸ ê³µì‚¬ì—…', '108'], ['ì‹¤ë‚´ì¥ì‹ ì§‘ì—ì„œ ë²½ì§€ ì¥íŒ ë“±ì„ ì´ìš©í•˜ì—¬ ë²½ì§€ ì¥íŒ íŒë§¤ ì‹œê³µ', '111'], ['ë‘ë¦¬ ìˆ˜ì‚°ì—ì„œ ì¼ë°˜ ì†Œë¹„ìì—ê²Œ ì†Œë§¤ ê°ì¢… ìƒì„  íŒ¬ ë§¥', '126'], ['ë‚´ê³¼ ì™¸ë˜í™˜ì ë‚´ê³¼ ì§„ë£Œ', '208'], ['ë¶€ë™ì‚° ì¤‘ê°œì†Œ ì¤‘ê°œ ë° ê³„ì•½ì„ ì²´ê²° ìˆ˜ìˆ˜ë£Œ ë°›ìŒ ë¶€ë™ì‚° ì¤‘ê°œ ì„œë¹„ìŠ¤', '169'], ['ê±´ì¶• ì„¤ê³„ ë° ê´€ë ¨ ì„œë¹„ìŠ¤ì—…  ê±´ì¶•ì„¤ê³„', '178'], ['ë¶€ë™ì‚° ì¤‘ê°œì†Œì—ì„œ ê³„ì•½ ë° ì¤‘ê°œì— ì˜í•´ ìˆ˜ìˆ˜ë£Œ ë°›ìŒ ë¶€ë™ì‚° ê±°ë˜ ì¤‘ê°œì„œë¹„ìŠ¤', '169'], ['ì»¤í”¼ì™€ ìŒë£Œ íŒë§¤  ', '147'], ['ì†œ ì›ë£Œë¥¼ ì¤‘í•©  ë°©ì‚¬ ', '48'], ['ì‚¬ì—…ì¥ì—ì„œ ì¼ë°˜ì¸ ë°©ë¬¸ê°ì„ í†µí•˜ì—¬ ëª…ìƒìˆ˜ë ¨ ì„œë¹„ìŠ¤', '205']]
    [['ì‹ ë°œê°€ê²Œì—ì„œ ì¼ë°˜ ì†Œë¹„ìì—ê²Œ ì†Œë§¤ ë‚¨ë…€ ì •ì¥í™”', '128'], ['ì²  ì…ê³   í‘œë©´ê°€ê³µ ', '63'], ['ìƒê°€ ê´€ë¦¬ì‚¬ë¬´ì†Œì—ì„œ ìƒê°€ ì…ì£¼ë¯¼ì„ ìœ„í•´ ìƒê°€ ê´€ë¦¬ ì„œë¹„ìŠ¤', '169'], ['ì •ìœ¡ì ì—ì„œ ì†Œë¹„ìë¥¼ ëŒ€ìƒìœ¼ë¡œ ì†Œê³ ê¸° ë¼ì§€ê³ ê¸°', '126'], ['ì‚¬ë¬´ì‹¤ì—ì„œ ì˜ë¢°ë¥¼ ë°›ì•„ ì„¸ë¬´ëŒ€í–‰', '173'], ['ê°•ìŠµì§±ì—ì„œ ì¼ë°˜ ê³ ê° ëŒ€ìƒìœ¼ë¡œ í•„ë¼í…ŒìŠ¤ ê°•ìŠµ', '205'], ['ëª¨í„°  ê°ì†ê¸° ì¡°ë¦½  ë„ì¥ ', '81'], ['ì‚°ì—… ì‚¬ìš©ìì—ê²Œ ì¦ê¸°', '99'], ['ëª¨í…” ê³ ê° ìˆ™ë°• ì„œë¹„ìŠ¤', '145'], ['ì •ìœ¡ì ì—ì„œ ì¼ë°˜ ì†Œë¹„ìì—ê²Œ ì†Œê³ ê¸°  ë¼ì§€ê³ ê¸° íŒë§¤', '147']]
    


```python
# KoBERT ì…ë ¥ ë°ì´í„°ë¡œ ë§Œë“¤ê¸°
# BERT ëª¨ë¸ì— ë“¤ì–´ê°€ê¸° ìœ„í•œ datasetì„ ë§Œë“¤ì–´ì£¼ëŠ” í´ë˜ìŠ¤
class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,
                 pad, pair):
        transform = nlp.data.BERTSentenceTransform(
            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)

        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i], ))

    def __len__(self):
        return (len(self.labels))
```


```python

```


```python
# Setting parameters
# ì´ê±´ ë‚˜ì¤‘ì— ìµœì í™” ê°’ ì°¾ì•„ë´ì•¼í•  ë“¯
max_len = 64
batch_size = 128
warmup_ratio = 0.1
num_epochs = 5
max_grad_norm = 1
log_interval = 200
learning_rate = 5e-5 # 0.0001 # 

# í† í°í™” ì‹¤í–‰
tokenizer = get_tokenizer()
tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)
```

    using cached model. /content/drive/MyDrive/nlp_c/eda_nlp/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece
    


```python
data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, pad = True, pair = False)
data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, pad = True, pair = False)
```


```python
sentence = dataset_train[30][0] 
print(sentence)
print(tok(sentence))

# í† í°í™” íŒ¨ë”© ì²˜ë¦¬ í›„ ê²°ê³¼ê°’ 
print(data_train[30])
```

    ìŒì‹ì ì—ì„œ ê°ì„ì„ ê°–ì¶”ê³  í•œì‹ íŒë§¤  
    ['â–ìŒì‹', 'ì ', 'ì—ì„œ', 'â–', 'ê°', 'ì„', 'ì„', 'â–ê°–ì¶”', 'ê³ ', 'â–í•œ', 'ì‹', 'â–íŒë§¤']
    (array([   2, 3609, 7220, 6903,  517, 5370, 6557, 7088,  827, 5439, 4955,
           6730, 4809,    3,    1,    1,    1,    1,    1,    1,    1,    1,
              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
              1,    1,    1,    1,    1,    1,    1,    1,    1], dtype=int32), array(14, dtype=int32), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
          dtype=int32), 147)
    


```python
# torch í˜•ì‹ì˜ dataset ìƒì„±
# num_worker ì€ gpu í™œì •í™” ì •ë„, 5ë¡œ í•˜ë‹ˆ ì˜¤íˆë ¤ ê³¼ë¶€í™”ê°€ ê±¸ë ¤ 4ë¡œ ì¡°ì •
train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=4)
test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=4)
```


```python
# í´ë˜ìŠ¤ ìˆ˜ ì¡°ì •
print(len(y_dict))
```

    225
    


```python
# KoBERT í•™ìŠµëª¨ë¸ ë§Œë“¤ê¸°
class BERTClassifier(nn.Module):
    def __init__(self,
                 bert,
                 hidden_size = 768,
                 num_classes=len(y_dict),   ##í´ë˜ìŠ¤ ìˆ˜ ì¡°ì •í•´ì¤˜ì•¼í•¨##
                 dr_rate=None,
                 params=None):
        super(BERTClassifier, self).__init__()
        self.bert = bert
        self.dr_rate = dr_rate
                 
        self.classifier = nn.Linear(hidden_size , num_classes)
        if dr_rate:
            self.dropout = nn.Dropout(p=dr_rate)
    
    def gen_attention_mask(self, token_ids, valid_length):
        attention_mask = torch.zeros_like(token_ids)
        for i, v in enumerate(valid_length):
            attention_mask[i][:v] = 1
        return attention_mask.float()

    def forward(self, token_ids, valid_length, segment_ids):
        attention_mask = self.gen_attention_mask(token_ids, valid_length)
        
        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))
        if self.dr_rate:
            out = self.dropout(pooler)
        return self.classifier(out)
```


```python
# GPU ì‹¤í–‰ ì˜¤ë¥˜ ë‚˜ë©´ ì‚¬ìš©
# import os
# os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
# os.environ["CUDA_VISIBLE_DEVICES"] = "0"
```


```python
model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)
```


```python
# Prepare optimizer and schedule (linear warmup and decay)
no_decay = ['bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
]
```


```python
optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)
loss_fn = nn.CrossEntropyLoss()
```

    /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
      FutureWarning,
    


```python
t_total = len(train_dataloader) * num_epochs
warmup_step = int(t_total * warmup_ratio)
```


```python
scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)
```


```python
def calc_accuracy(X,Y):
    max_vals, max_indices = torch.max(X, 1)
    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]
    return train_acc
```


```python
# pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html
```


```python
highest_acc = 0
patience = 0

# ìµœì¢… ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸°
for e in range(num_epochs):
    train_acc = 0.0
    test_acc = 0.0
    model.train()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):
        optimizer.zero_grad()
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)
        loss = loss_fn(out, label)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
        scheduler.step()  # Update learning rate schedule
        train_acc += calc_accuracy(out, label)
        if batch_id % log_interval == 0:
            print("epoch {} batch id {} loss {} train acc {}".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))
    print("epoch {} train acc {}".format(e+1, train_acc / (batch_id+1)))
    
    model.eval()

    for test_batch_id, (test_token_ids, test_valid_length, test_segment_ids, test_label) in enumerate(tqdm_notebook(test_dataloader)):
        test_token_ids = test_token_ids.long().to(device)
        test_segment_ids = test_segment_ids.long().to(device)
        test_valid_length= test_valid_length
        test_label = test_label.long().to(device)
        test_out = model(token_ids, valid_length, segment_ids)
        test_loss = loss_fn(out, label)
        test_acc += calc_accuracy(out, label)
    print("epoch {} test acc {}".format(e+1, test_acc / (test_batch_id+1)))

    if test_acc > highest_acc:
        torch.save({
            'epoch': e,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': test_loss,
            }, path + 'final_data/' + 'correct_model_fin.pt')
        patience = 0
    else:
        print("test acc did not improved. best:{} current:{}".format(highest_acc, test_acc))
        patience += 1
        if patience > 5:
            break
    print('current patience: {}'.format(patience))
    print("************************************************************************************")
```

    /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0
    Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`
      if __name__ == '__main__':
    


      0%|          | 0/4800 [00:00<?, ?it/s]


    epoch 1 batch id 1 loss 5.470526218414307 train acc 0.0
    epoch 1 batch id 201 loss 5.183148384094238 train acc 0.04287157960199005
    epoch 1 batch id 401 loss 4.4437031745910645 train acc 0.08350218204488778
    epoch 1 batch id 601 loss 3.7217705249786377 train acc 0.13737520798668884
    epoch 1 batch id 801 loss 3.2203402519226074 train acc 0.19774110486891386
    epoch 1 batch id 1001 loss 2.5548582077026367 train acc 0.2525131118881119
    epoch 1 batch id 1201 loss 2.060025215148926 train acc 0.3013049021648626
    epoch 1 batch id 1401 loss 1.7654931545257568 train acc 0.3442184154175589
    epoch 1 batch id 1601 loss 1.7790783643722534 train acc 0.3823538023110556
    epoch 1 batch id 1801 loss 1.3805230855941772 train acc 0.415585091615769
    epoch 1 batch id 2001 loss 1.0565495491027832 train acc 0.445050912043978
    epoch 1 batch id 2201 loss 1.0683343410491943 train acc 0.4712275670149932
    epoch 1 batch id 2401 loss 1.3318103551864624 train acc 0.49479708975426906
    epoch 1 batch id 2601 loss 0.9846600294113159 train acc 0.5164059496347558
    epoch 1 batch id 2801 loss 0.9582300782203674 train acc 0.5354588762941807
    epoch 1 batch id 3001 loss 0.7929444313049316 train acc 0.5528860171609463
    epoch 1 batch id 3201 loss 0.6908536553382874 train acc 0.5685332708528584
    epoch 1 batch id 3401 loss 0.7846947312355042 train acc 0.5832521684798588
    epoch 1 batch id 3601 loss 0.6151464581489563 train acc 0.5966419918078312
    epoch 1 batch id 3801 loss 0.6870463490486145 train acc 0.6090009208103131
    epoch 1 batch id 4001 loss 0.516919732093811 train acc 0.6200266339665084
    epoch 1 batch id 4201 loss 0.8247902393341064 train acc 0.6302535854558439
    epoch 1 batch id 4401 loss 0.7708945870399475 train acc 0.6398030987275619
    epoch 1 batch id 4601 loss 0.7333303689956665 train acc 0.648410331993045
    epoch 1 train acc 0.6567227647569445
    

    /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0
    Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`
    


      0%|          | 0/1600 [00:00<?, ?it/s]


    epoch 1 test acc 0.9333333333333352
    current patience: 0
    ************************************************************************************
    


      0%|          | 0/4800 [00:00<?, ?it/s]


    epoch 2 batch id 1 loss 0.6638566255569458 train acc 0.8359375
    epoch 2 batch id 201 loss 0.6112797260284424 train acc 0.8494247512437811
    epoch 2 batch id 401 loss 0.7496534585952759 train acc 0.8484842581047382
    epoch 2 batch id 601 loss 0.6063464879989624 train acc 0.8506395590682196
    epoch 2 batch id 801 loss 0.795397937297821 train acc 0.8525963639200999
    epoch 2 batch id 1001 loss 0.429190456867218 train acc 0.853279532967033
    epoch 2 batch id 1201 loss 0.6381816267967224 train acc 0.854001873438801
    epoch 2 batch id 1401 loss 0.4390967786312103 train acc 0.855638829407566
    epoch 2 batch id 1601 loss 0.5170191526412964 train acc 0.8566667317301686
    epoch 2 batch id 1801 loss 0.35547658801078796 train acc 0.8579348278734037
    epoch 2 batch id 2001 loss 0.3872775733470917 train acc 0.8594374687656172
    epoch 2 batch id 2201 loss 0.48807698488235474 train acc 0.8607344672875965
    epoch 2 batch id 2401 loss 0.6643775105476379 train acc 0.8618381663890046
    epoch 2 batch id 2601 loss 0.47221654653549194 train acc 0.8632617262591311
    epoch 2 batch id 2801 loss 0.43773651123046875 train acc 0.8640468805783649
    epoch 2 batch id 3001 loss 0.3916258215904236 train acc 0.8651985796401199
    epoch 2 batch id 3201 loss 0.4372307360172272 train acc 0.866157548422368
    epoch 2 batch id 3401 loss 0.4276728928089142 train acc 0.8671002094972067
    epoch 2 batch id 3601 loss 0.3759610652923584 train acc 0.8680531449597334
    epoch 2 batch id 3801 loss 0.5400181412696838 train acc 0.8691318896343068
    epoch 2 batch id 4001 loss 0.2910200357437134 train acc 0.8699446232191952
    epoch 2 batch id 4201 loss 0.6544965505599976 train acc 0.8706911300880743
    epoch 2 batch id 4401 loss 0.5287626385688782 train acc 0.8716183111792775
    epoch 2 batch id 4601 loss 0.5366830825805664 train acc 0.872120191262769
    epoch 2 train acc 0.8730557725694444
    


      0%|          | 0/1600 [00:00<?, ?it/s]


    epoch 2 test acc 0.9333333333333352
    current patience: 0
    ************************************************************************************
    


      0%|          | 0/4800 [00:00<?, ?it/s]


    epoch 3 batch id 1 loss 0.47743237018585205 train acc 0.859375
    epoch 3 batch id 201 loss 0.4529868960380554 train acc 0.8904306592039801
    epoch 3 batch id 401 loss 0.46464189887046814 train acc 0.8910536159600998
    epoch 3 batch id 601 loss 0.44063830375671387 train acc 0.8932378327787022
    epoch 3 batch id 801 loss 0.4417380094528198 train acc 0.8939509207240949
    epoch 3 batch id 1001 loss 0.32520782947540283 train acc 0.8938951673326674
    epoch 3 batch id 1201 loss 0.4275073707103729 train acc 0.8943263426311407
    epoch 3 batch id 1401 loss 0.3466779887676239 train acc 0.895130710206995
    epoch 3 batch id 1601 loss 0.3258431553840637 train acc 0.8953974078700812
    epoch 3 batch id 1801 loss 0.339708536863327 train acc 0.8959171987784564
    epoch 3 batch id 2001 loss 0.36911335587501526 train acc 0.8968250249875063
    epoch 3 batch id 2201 loss 0.38383665680885315 train acc 0.8975039754656974
    epoch 3 batch id 2401 loss 0.5033802390098572 train acc 0.898001483756768
    epoch 3 batch id 2601 loss 0.3143329322338104 train acc 0.8987919309880815
    epoch 3 batch id 2801 loss 0.30946600437164307 train acc 0.8991515307033202
    epoch 3 batch id 3001 loss 0.2630763053894043 train acc 0.8998667110963012
    epoch 3 batch id 3201 loss 0.36716964840888977 train acc 0.900250898156826
    epoch 3 batch id 3401 loss 0.3716908097267151 train acc 0.9007713723904733
    epoch 3 batch id 3601 loss 0.2624480426311493 train acc 0.9013360004165509
    epoch 3 batch id 3801 loss 0.3792986571788788 train acc 0.9020056399631676
    epoch 3 batch id 4001 loss 0.23668766021728516 train acc 0.9024404055236191
    epoch 3 batch id 4201 loss 0.5264573097229004 train acc 0.9028598101642467
    epoch 3 batch id 4401 loss 0.3679966628551483 train acc 0.903464766530334
    epoch 3 batch id 4601 loss 0.32983699440956116 train acc 0.9037607313627473
    epoch 3 train acc 0.9043473307291666
    


      0%|          | 0/1600 [00:00<?, ?it/s]


    epoch 3 test acc 1.0
    current patience: 0
    ************************************************************************************
    


      0%|          | 0/4800 [00:00<?, ?it/s]


    epoch 4 batch id 1 loss 0.344111829996109 train acc 0.8828125
    epoch 4 batch id 201 loss 0.36581531167030334 train acc 0.915967039800995
    epoch 4 batch id 401 loss 0.38386818766593933 train acc 0.9154652431421446
    epoch 4 batch id 601 loss 0.38915520906448364 train acc 0.9167013311148087
    epoch 4 batch id 801 loss 0.38615915179252625 train acc 0.9174274344569289
    epoch 4 batch id 1001 loss 0.2394852340221405 train acc 0.917239010989011
    epoch 4 batch id 1201 loss 0.2488376498222351 train acc 0.9173865528726062
    epoch 4 batch id 1401 loss 0.22874371707439423 train acc 0.9179101980728052
    epoch 4 batch id 1601 loss 0.2878819704055786 train acc 0.9182200577763897
    epoch 4 batch id 1801 loss 0.21825024485588074 train acc 0.9187777623542477
    epoch 4 batch id 2001 loss 0.2466854304075241 train acc 0.9195792728635682
    epoch 4 batch id 2201 loss 0.26348188519477844 train acc 0.9201250851885506
    epoch 4 batch id 2401 loss 0.4421859681606293 train acc 0.9205181434818825
    epoch 4 batch id 2601 loss 0.23925673961639404 train acc 0.9212292147251058
    epoch 4 batch id 2801 loss 0.24297061562538147 train acc 0.9213338986076401
    epoch 4 batch id 3001 loss 0.17798040807247162 train acc 0.9218567769076974
    epoch 4 batch id 3201 loss 0.260630339384079 train acc 0.9221703178694158
    epoch 4 batch id 3401 loss 0.26668980717658997 train acc 0.9225090047044987
    epoch 4 batch id 3601 loss 0.1432698667049408 train acc 0.9228729866703693
    epoch 4 batch id 3801 loss 0.27312996983528137 train acc 0.9232644369902657
    epoch 4 batch id 4001 loss 0.21531520783901215 train acc 0.9235445044988753
    epoch 4 batch id 4201 loss 0.4101676642894745 train acc 0.9238406778148059
    epoch 4 batch id 4401 loss 0.27769315242767334 train acc 0.9242324187684617
    epoch 4 batch id 4601 loss 0.26794159412384033 train acc 0.9242997446207346
    epoch 4 train acc 0.9246866319444444
    


      0%|          | 0/1600 [00:00<?, ?it/s]


    epoch 4 test acc 0.9333333333333352
    current patience: 0
    ************************************************************************************
    


      0%|          | 0/4800 [00:00<?, ?it/s]


    epoch 5 batch id 1 loss 0.2737545073032379 train acc 0.9140625
    epoch 5 batch id 201 loss 0.2553758919239044 train acc 0.9324860074626866
    epoch 5 batch id 401 loss 0.2817900776863098 train acc 0.9322007481296758
    epoch 5 batch id 601 loss 0.2567768692970276 train acc 0.9328462978369384
    epoch 5 batch id 801 loss 0.2468549758195877 train acc 0.9336278870162297
    epoch 5 batch id 1001 loss 0.22222311794757843 train acc 0.9333791208791209
    epoch 5 batch id 1201 loss 0.25492382049560547 train acc 0.9332717527060783
    epoch 5 batch id 1401 loss 0.15025579929351807 train acc 0.9334515524625268
    epoch 5 batch id 1601 loss 0.22277267277240753 train acc 0.933493714865709
    epoch 5 batch id 1801 loss 0.19275978207588196 train acc 0.9338171501943365
    epoch 5 batch id 2001 loss 0.22030514478683472 train acc 0.9345483508245878
    epoch 5 batch id 2201 loss 0.21622973680496216 train acc 0.9348627044525216
    epoch 5 batch id 2401 loss 0.3563814163208008 train acc 0.9350205643481883
    epoch 5 batch id 2601 loss 0.2274296134710312 train acc 0.9353794213763937
    epoch 5 batch id 2801 loss 0.2206992506980896 train acc 0.9353300160656908
    epoch 5 batch id 3001 loss 0.12332244217395782 train acc 0.9355943852049317
    epoch 5 batch id 3201 loss 0.22073575854301453 train acc 0.9356060606060606
    epoch 5 batch id 3401 loss 0.20686203241348267 train acc 0.9357036533372537
    epoch 5 batch id 3601 loss 0.1583988219499588 train acc 0.9359314252985282
    epoch 5 batch id 3801 loss 0.2532764673233032 train acc 0.9360961753485925
    epoch 5 batch id 4001 loss 0.15411587059497833 train acc 0.9361585384903774
    epoch 5 batch id 4201 loss 0.30796435475349426 train acc 0.9362651749583433
    epoch 5 batch id 4401 loss 0.23673692345619202 train acc 0.9364260253351511
    epoch 5 batch id 4601 loss 0.23493117094039917 train acc 0.9363640377091936
    epoch 5 train acc 0.9365494791666666
    


      0%|          | 0/1600 [00:00<?, ?it/s]


    epoch 5 test acc 1.0
    current patience: 0
    ************************************************************************************
    


```python
# # í•™ìŠµí•œ ëª¨ë¸ pickle í˜•íƒœë¡œ ì €ì¥

# import pickle
# # path = '/content/drive/MyDrive/nlp_c/'

# with open(path+'model_trial_fin_noclean.pickle', 'wb') as f:
#     pickle.dump(model, f)
```


```python

```

# **ì½”ë“œë¶ìœ¼ë¡œ ì œì¶œí˜•ì‹ ë§Œë“¤ê¸°**

## í•œêµ­í‘œì¤€ì‚°ì—…ë¶„ë¥˜(10ì°¨)_êµ­ë¬¸ ìë£Œ ì´ìš©í•´ì„œ ì½”ë“œ ë¶ ë§Œë“¤ê¸°


```python
import numpy as np 
import pandas as pd
```


```python
path = '/content/drive/MyDrive/nlp_c/'
code_book = pd.read_excel(path + 'á„’á…¡á†«á„€á…®á†¨á„‘á…­á„Œá…®á†«á„‰á…¡á†«á„‹á…¥á†¸á„‡á…®á†«á„…á…²(10á„á…¡)_á„€á…®á†¨á„†á…®á†«.xlsx', header = 1)
```


```python
code_book = code_book.dropna(subset = ['ì†Œë¶„ë¥˜(232)'])
code_book[:10]
```





  <div id="df-c488b6e9-5c18-4599-bd39-cd5701d74f41">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ëŒ€ë¶„ë¥˜(21)</th>
      <th>Unnamed: 1</th>
      <th>ì¤‘ë¶„ë¥˜(77)</th>
      <th>Unnamed: 3</th>
      <th>ì†Œë¶„ë¥˜(232)</th>
      <th>Unnamed: 5</th>
      <th>ì„¸ë¶„ë¥˜(495)</th>
      <th>Unnamed: 7</th>
      <th>ì„¸ì„¸ë¶„ë¥˜(1,196)</th>
      <th>Unnamed: 9</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ì½”ë“œ</td>
      <td>í•­ëª©ëª…</td>
      <td>ì½”ë“œ</td>
      <td>í•­ëª©ëª…</td>
      <td>ì½”ë“œ</td>
      <td>í•­ëª©ëª…</td>
      <td>ì½”ë“œ</td>
      <td>í•­ëª©ëª…</td>
      <td>ì½”ë“œ</td>
      <td>í•­ëª©ëª…</td>
    </tr>
    <tr>
      <th>1</th>
      <td>A</td>
      <td>ë†ì—…, ì„ì—… ë° ì–´ì—…(01~03)</td>
      <td>01</td>
      <td>ë†ì—…</td>
      <td>011</td>
      <td>ì‘ë¬¼ ì¬ë°°ì—…</td>
      <td>0111</td>
      <td>ê³¡ë¬¼ ë° ê¸°íƒ€ ì‹ëŸ‰ì‘ë¬¼ ì¬ë°°ì—…</td>
      <td>01110</td>
      <td>ê³¡ë¬¼ ë° ê¸°íƒ€ ì‹ëŸ‰ì‘ë¬¼ ì¬ë°°ì—…</td>
    </tr>
    <tr>
      <th>11</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>012</td>
      <td>ì¶•ì‚°ì—…</td>
      <td>0121</td>
      <td>ì†Œ ì‚¬ìœ¡ì—…</td>
      <td>01211</td>
      <td>ì –ì†Œ ì‚¬ìœ¡ì—…</td>
    </tr>
    <tr>
      <th>18</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>013</td>
      <td>ì‘ë¬¼ì¬ë°° ë° ì¶•ì‚° ë³µí•©ë†ì—…</td>
      <td>0130</td>
      <td>ì‘ë¬¼ì¬ë°° ë° ì¶•ì‚° ë³µí•©ë†ì—…</td>
      <td>01300</td>
      <td>ì‘ë¬¼ì¬ë°° ë° ì¶•ì‚° ë³µí•©ë†ì—…</td>
    </tr>
    <tr>
      <th>19</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>014</td>
      <td>ì‘ë¬¼ì¬ë°° ë° ì¶•ì‚° ê´€ë ¨ ì„œë¹„ìŠ¤ì—…</td>
      <td>0141</td>
      <td>ì‘ë¬¼ì¬ë°° ê´€ë ¨ ì„œë¹„ìŠ¤ì—…</td>
      <td>01411</td>
      <td>ì‘ë¬¼ì¬ë°° ì§€ì› ì„œë¹„ìŠ¤ì—…</td>
    </tr>
    <tr>
      <th>22</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>015</td>
      <td>ìˆ˜ë µ ë° ê´€ë ¨ ì„œë¹„ìŠ¤ì—…</td>
      <td>0150</td>
      <td>ìˆ˜ë µ ë° ê´€ë ¨ ì„œë¹„ìŠ¤ì—…</td>
      <td>01500</td>
      <td>ìˆ˜ë µ ë° ê´€ë ¨ ì„œë¹„ìŠ¤ì—…</td>
    </tr>
    <tr>
      <th>23</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>02</td>
      <td>ì„ì—…</td>
      <td>020</td>
      <td>ì„ì—…</td>
      <td>0201</td>
      <td>ì˜ë¦¼ì—…</td>
      <td>02011</td>
      <td>ì„ì—…ìš© ì¢…ë¬˜ ìƒì‚°ì—…</td>
    </tr>
    <tr>
      <th>28</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>03</td>
      <td>ì–´ì—…</td>
      <td>031</td>
      <td>ì–´ë¡œ ì–´ì—…</td>
      <td>0311</td>
      <td>í•´ìˆ˜ë©´ ì–´ì—…</td>
      <td>03111</td>
      <td>ì›ì–‘ ì–´ì—…</td>
    </tr>
    <tr>
      <th>31</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>032</td>
      <td>ì–‘ì‹ì–´ì—… ë° ì–´ì—…ê´€ë ¨ ì„œë¹„ìŠ¤ì—…</td>
      <td>0321</td>
      <td>ì–‘ì‹ ì–´ì—…</td>
      <td>03211</td>
      <td>í•´ìˆ˜ë©´ ì–‘ì‹ ì–´ì—…</td>
    </tr>
    <tr>
      <th>35</th>
      <td>B</td>
      <td>ê´‘ì—…(05~08)</td>
      <td>05</td>
      <td>ì„íƒ„, ì›ìœ  ë° ì²œì—°ê°€ìŠ¤ ê´‘ì—…</td>
      <td>051</td>
      <td>ì„íƒ„ ê´‘ì—…</td>
      <td>0510</td>
      <td>ì„íƒ„ ê´‘ì—…</td>
      <td>05100</td>
      <td>ì„íƒ„ ê´‘ì—…</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-c488b6e9-5c18-4599-bd39-cd5701d74f41')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-c488b6e9-5c18-4599-bd39-cd5701d74f41 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-c488b6e9-5c18-4599-bd39-cd5701d74f41');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python
code = code_book[['ëŒ€ë¶„ë¥˜(21)', 'ì¤‘ë¶„ë¥˜(77)', 'ì†Œë¶„ë¥˜(232)', 'Unnamed: 5']][1:].reset_index(drop=True)
```


```python
def na_to_code(data):
  data_l = []
  temp = data[0]

  for i in range(0, len(data)):
    if pd.isna(data[i]):
      data[i] =  temp
    else:
      temp = data[i] 
    data_l.append(temp)
  return data_l
```


```python
big = na_to_code(code.iloc[:,0].tolist())
middle = na_to_code(code.iloc[:,1].tolist())
small = na_to_code(code.iloc[:,2].tolist())
```


```python
code_b = pd.DataFrame(zip(big,middle,small), columns = ['big', 'middle', 'small'])
code_b['y'] = code_b['small'].astype('int64')
code_b['name'] = code['Unnamed: 5']
code_b
code_b.to_excel(path + 'codebook_dict.xlsx', index=False, encoding = 'EUC-KR')
```


```python
code_b = pd.read_excel(path + 'codebook_dict.xlsx', dtype = {'big': str, 'middle': str, 'small': str})
```


```python
code_b = code_b.iloc[:,:-1] # nameì€ ì°¸ê³ ìš©ì´ë¯€ë¡œ ì œì™¸
dict_fin = code_b.set_index('y').T.to_dict('list') # ì†Œë¶„ë¥˜ê°’ì„ key ë¡œ í•œ dictionary
```

## í•™ìŠµí•œ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ë° í™˜ê²½ êµ¬ì¶•


```python
# # train í•  ë•Œ, ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©í•˜ì—¬, ë¹„ìš°ê¸°
# import gc
# gc.collect()
# del model
# torch.cuda.empty_cache()
```


```python
# test í•˜ê¸°ì „ ëª¨ë¸ ê¸°ë³¸ ê°’ ë¶ˆëŸ¬ì˜¤ê¸°

# KoBERT ì…ë ¥ ë°ì´í„°ë¡œ ë§Œë“¤ê¸°
# BERT ëª¨ë¸ì— ë“¤ì–´ê°€ê¸° ìœ„í•œ datasetì„ ë§Œë“¤ì–´ì£¼ëŠ” í´ë˜ìŠ¤
class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,
                 pad, pair):
        transform = nlp.data.BERTSentenceTransform(
            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)

        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i], ))

    def __len__(self):
        return (len(self.labels))

# í† í°í™” ì‹¤í–‰
tokenizer = get_tokenizer()
tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)

# KoBERT í•™ìŠµëª¨ë¸ ë§Œë“¤ê¸°
class BERTClassifier(nn.Module):
    def __init__(self,
                 bert,
                 hidden_size = 768,
                 num_classes=225,   ##í´ë˜ìŠ¤ ìˆ˜ ì¡°ì •í•´ì¤˜ì•¼í•¨##
                 dr_rate=None,
                 params=None):
        super(BERTClassifier, self).__init__()
        self.bert = bert
        self.dr_rate = dr_rate
                 
        self.classifier = nn.Linear(hidden_size , num_classes)
        if dr_rate:
            self.dropout = nn.Dropout(p=dr_rate)
    
    def gen_attention_mask(self, token_ids, valid_length):
        attention_mask = torch.zeros_like(token_ids)
        for i, v in enumerate(valid_length):
            attention_mask[i][:v] = 1
        return attention_mask.float()

    def forward(self, token_ids, valid_length, segment_ids):
        attention_mask = self.gen_attention_mask(token_ids, valid_length)
        
        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))
        if self.dr_rate:
            out = self.dropout(pooler)
        return self.classifier(out)
```

    using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece
    


```python
# import pickle
# # í•™ìŠµí•œ model ì—´ê¸°

# with open(path+'model_trial_fin_noclean.pickle', 'rb') as f:
#     model = pickle.load(f)
```


```python
# Setting parameters
# ì´ê±´ ë‚˜ì¤‘ì— ìµœì í™” ê°’ ì°¾ì•„ë´ì•¼í•  ë“¯
max_len = 64
batch_size = 128
warmup_ratio = 0.1
num_epochs = 6
max_grad_norm = 1
log_interval = 200
learning_rate = 5e-5 # 0.0001 # 
```


```python
model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)
# Prepare optimizer and schedule (linear warmup and decay)
no_decay = ['bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
]
optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)

checkpoint = torch.load(path + 'final_data/' + 'correct_model_fin.pt') # í•™ìŠµí•œ íŒŒì¼ ê²½ë¡œ ì§€ì •
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']
```

    /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
      FutureWarning,
    


```python
model.eval()
```




    BERTClassifier(
      (bert): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(8002, 768, padding_idx=1)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
      (classifier): Linear(in_features=768, out_features=225, bias=True)
      (dropout): Dropout(p=0.5, inplace=False)
    )



## ë§ì¶¤ë²• ì²˜ë¦¬í•œ ì œì¶œíŒŒì¼ ì œì¶œ í˜•ì‹ìœ¼ë¡œ ë°”ê¾¸ê¸°


```python
test = pd.read_csv(path + 'final_data/' + 'correct_sub_fin.csv', encoding = 'utf-8-sig')
```


```python
dataset_test = [[str(a), '0'] for a in test['clean_done']]
dataset_test[:20]
```




    [['ì¹˜í‚¨ì „ë¬¸ì ì—ì„œ ê³ ê°ì˜ ì£¼ë¬¸ì— ì˜í•´ ì¹˜í‚¨ íŒë§¤', '0'],
     ['ì‚°ì—…ê³µêµ¬ ë‹¤ë¥¸ ì†Œë§¤ì—…ìì—ê²Œ ì² ë¬¼ ìˆ˜ê³µêµ¬', '0'],
     ['ì ˆì—ì„œ ì‹ ë„ë¥¼ ëŒ€ìƒìœ¼ë¡œ ë¶ˆêµë‹¨ì²´ ìš´ì˜', '0'],
     ['ì˜ì—…ì¥ì—ì„œ ê³ ê° ìš”êµ¬ë¡œ ìë™ì°¨ íŠœë‹', '0'],
     ['ì‹¤ë‚´í¬ì¥ë§ˆì°¨ì—ì„œ ì ‘ê°ì‹œì„¤ì„ ê°–ì¶”ê³  ì†Œì£¼ ë§¥ì£¼ ì œê³µ', '0'],
     ['ì²  ì•„í¬ë¦´ ì½”ë§¥ìŠ¤ ìŠ¤í¬ë¦°ì¸ì‡„ ëª…íŒ', '0'],
     ['ìŒì‹ì  ì ‘ê°ì‹œì„¤ ê°€ì§€ê³  ì¡° ê°œêµ¬ ì´ íŒë§¤', '0'],
     ['ìŠ¤í…Œì¸ë¦¬ìŠ¤ë¥¼ í”„ë ˆìŠ¤ê°€ê³µí•˜ì—¬ ì œì¡° ì£¼ë°©ìš©í’ˆ', '0'],
     ['ìˆ˜ë¦¬ ì„œë¹„ìŠ¤ì„¼í„°ì—ì„œ ì „ë¬¸ìˆ˜ë¦¬ ìˆ˜ì…ì°¨', '0'],
     ['ì•½í’ˆ í™”ê³µ   ë¯¸ì‹± ì™„ì„±í’ˆ ì…ê³   ìˆ˜ì„  ', '0'],
     ['ë°€ê°€ë£¨  ì‡¼íŠ¸ë‹ ì›ì¬ë£Œ ì…ê³   ë°˜ì£½ ', '0'],
     ['ì´ë°œì†Œì—ì„œ ì¼ë°˜ì¸ ëŒ€ìƒìœ¼ë¡œ ê³ ê°ì˜ ë‘ë°œì„ ì†ì§ˆí•¨', '0'],
     ['ëª©ì¬ ê³ ê°ì˜ ìš”êµ¬ì— ë”°ë¼ ë¬´ëŒ€ì¥ì¹˜', '0'],
     ['ì˜ì›ì—ì„œ ì†Œì•„ ì²­ì†Œë…„ì„ ëŒ€ìƒìœ¼ë¡œ ì§„ë£Œì„œë¹„ìŠ¤', '0'],
     ['ì‚°ì—…ê³µë‹¨ ì¡°ì„± ì‹œ ì˜ë¢°ë°›ì•„ ì‚¬ë¬´ì‹¤ì—ì„œ ì¸¡ëŸ‰ í† ëª©ì„¤ê³„', '0'],
     ['íƒœêµ­ ì „í†µ ë§ˆì‚¬ì§€ìˆ ì¼ë°˜ ê³ ê° ëŒ€ìƒ ë§ˆì‚¬ì§€', '0'],
     ['ì‚¬ì—…ì¥ì—ì„œ ê´€ë ¨ ì‚¬ìš©ìì—ê²Œ ì„ ë°•ì—”ì§„ë¶€í’ˆ', '0'],
     ['ìƒê°€ì—ì„œ ì£¼ë¬¸ì— ì˜í•´ ë°˜ì°¬ ë„ì‹œë½', '0'],
     ['ë¬¸êµ¬ìš©í’ˆì—ì„œ ì¼ë°˜ì¸ ëŒ€ìƒ ë¬¸êµ¬ë¥˜ ì‚¬ë¬´ìš©í’ˆ ì†Œë§¤', '0'],
     ['ì—”ì§€ë‹ˆì–´ë§ ë° ê±´ì¶• ê´€ë ¨ ê³ ê° ìš”ì²­ì— ì˜í•´ ë„ë©´ ì„¤ê³„', '0']]




```python
from tqdm.notebook import tqdm
```


```python
# ì˜ˆì¸¡ í•¨ìˆ˜ ìƒì„±
# Setting parameters
# train í•™ìŠµ ëª¨ë¸ ì„¤ì •í•  ë•Œì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
def predict_set(dataset_test):

    test_acc = 0.0

    tokenizer = get_tokenizer()
    tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)

    data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)

    test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=4)

    out_list =[]

    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)

        valid_length= valid_length
        label = label.long().to(device)

        out = model(token_ids, valid_length, segment_ids)
        output = out.detach().cpu().tolist()
        out_list.append(output)

    pd = sum(out_list,[])
    pd_list = pd_list = [np.argmax(i) for i in pd]
    return pd_list
```


```python
p_test = predict_set(dataset_test)
```

    using cached model. /content/drive/MyDrive/nlp_c/eda_nlp/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece
    


      0%|          | 0/782 [00:00<?, ?it/s]



```python
import pickle
# í•™ìŠµí•˜ê¸°ì „ ê¸°ì¡´ yê°’ ì‚¬ì „ ì—´ê¸°
with open(path+'final_data/' + 's_dictionary', 'rb') as f:
    s_dict = pickle.load(f)
```


```python
test['predict_y'] = p_test
# 1. ëª¨ë¸ í•™ìŠµí•˜ê¸° ì „ ê¸°ì¡´ yê°’ ë³€ìˆ˜ë¡œ ë³€í™˜
test['predict_y'] = test['predict_y'].astype('str').map(s_dict) 
cols = ['digit_1', 'digit_2', 'digit_3']
# 2. ì½”ë“œë¶ì—ì„œ ì†Œë¶„ë¥˜ë¥¼ í†µí•´ ëŒ€/ì¤‘ë¶„ë¥˜ í•¨ê»˜ ì˜ˆì¸¡
test[cols] = test['predict_y'].astype('int64').map(dict_fin).apply(lambda x: pd.Series(x))
```


```python
test_fin = test[['AI_id', 'digit_1', 'digit_2', 'digit_3', 'text_obj', 'text_mthd', 'text_deal']]
test_fin.to_csv(path + 'final_data/' + 'submission_fin_0413.csv', index=False, encoding='EUC-KR')
```


```python
test_fin[:50]
```





  <div id="df-ea3534e9-4f75-4ad2-aaea-09d6613cf35c">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AI_id</th>
      <th>digit_1</th>
      <th>digit_2</th>
      <th>digit_3</th>
      <th>text_obj</th>
      <th>text_mthd</th>
      <th>text_deal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>id_000001</td>
      <td>I</td>
      <td>56</td>
      <td>561</td>
      <td>ì¹˜í‚¨ì „ë¬¸ì ì—ì„œ</td>
      <td>ê³ ê°ì˜ì£¼ë¬¸ì—ì˜í•´</td>
      <td>ì¹˜í‚¨íŒë§¤</td>
    </tr>
    <tr>
      <th>1</th>
      <td>id_000002</td>
      <td>G</td>
      <td>46</td>
      <td>466</td>
      <td>ì‚°ì—…ê³µêµ¬</td>
      <td>ë‹¤ë¥¸ ì†Œë§¤ì—…ìì—ê²Œ</td>
      <td>ì² ë¬¼ ìˆ˜ê³µêµ¬</td>
    </tr>
    <tr>
      <th>2</th>
      <td>id_000003</td>
      <td>S</td>
      <td>94</td>
      <td>949</td>
      <td>ì ˆì—ì„œ</td>
      <td>ì‹ ë„ì„ ëŒ€ìƒìœ¼ë¡œ</td>
      <td>ë¶ˆêµë‹¨ì²´ìš´ì˜</td>
    </tr>
    <tr>
      <th>3</th>
      <td>id_000004</td>
      <td>S</td>
      <td>95</td>
      <td>952</td>
      <td>ì˜ì—…ì¥ì—ì„œ</td>
      <td>ê³ ê°ìš”êµ¬ë¡œ</td>
      <td>ìë™ì°¨íŠœë‹</td>
    </tr>
    <tr>
      <th>4</th>
      <td>id_000005</td>
      <td>I</td>
      <td>56</td>
      <td>562</td>
      <td>ì‹¤ë‚´í¬ì¥ë§ˆì°¨ì—ì„œ</td>
      <td>ì ‘ê°ì‹œì„¤ì„ ê°–ì¶”ê³ </td>
      <td>ì†Œì£¼,ë§¥ì£¼ì œê³µ</td>
    </tr>
    <tr>
      <th>5</th>
      <td>id_000006</td>
      <td>C</td>
      <td>18</td>
      <td>181</td>
      <td>ì² ,ì•„í¬ë¦´,í¬ë§¥ìŠ¤</td>
      <td>ìŠ¤í¬ë¦°ì¸ì‡„</td>
      <td>ëª…íŒ</td>
    </tr>
    <tr>
      <th>6</th>
      <td>id_000007</td>
      <td>I</td>
      <td>56</td>
      <td>561</td>
      <td>ìŒì‹ì </td>
      <td>ì ‘ê°ì‹œì„¤ê°€ì§€ê³ </td>
      <td>ì¡°ê°œêµ¬ì´íŒë§¤</td>
    </tr>
    <tr>
      <th>7</th>
      <td>id_000008</td>
      <td>C</td>
      <td>25</td>
      <td>259</td>
      <td>ìŠ¤í…Œì¸ë ˆìŠ¤ë¥¼</td>
      <td>í”„ë ˆìŠ¤ê°€ê³µí•˜ì—¬ì œì¡°</td>
      <td>ì£¼ë°©ìš©í’ˆ</td>
    </tr>
    <tr>
      <th>8</th>
      <td>id_000009</td>
      <td>S</td>
      <td>95</td>
      <td>952</td>
      <td>ìˆ˜ë¦¬</td>
      <td>ì„œë¹„ìŠ¤ì„¼í„°ì—ì„œ</td>
      <td>ì „ë¬¸ìˆ˜ë¦¬ ìˆ˜ì…ì°¨</td>
    </tr>
    <tr>
      <th>9</th>
      <td>id_000010</td>
      <td>C</td>
      <td>25</td>
      <td>259</td>
      <td>ì•½í’ˆ(í™”ê³µ), ë¯¸ì‹±</td>
      <td>ì™„ì„±í’ˆì…ê³ , ìˆ˜ì„ </td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>10</th>
      <td>id_000011</td>
      <td>C</td>
      <td>10</td>
      <td>107</td>
      <td>ë°€ê°€ë£¨, ì‡¼íŠ¸ë‹</td>
      <td>ì›ì¬ë£Œì…ê³ , ë°˜ì£½</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>11</th>
      <td>id_000012</td>
      <td>S</td>
      <td>96</td>
      <td>961</td>
      <td>ì´ë°œì†Œì—ì„œ</td>
      <td>ì¼ë°˜ì¸ ëŒ€ìƒìœ¼ë¡œ</td>
      <td>ê³ ê°ì˜ ë‘ë°œì„ ì†ì§ˆí•¨</td>
    </tr>
    <tr>
      <th>12</th>
      <td>id_000013</td>
      <td>C</td>
      <td>16</td>
      <td>162</td>
      <td>ëª©ì¬</td>
      <td>ê³ ê°ì˜ ìš”êµ¬ì— ë”°ë¼</td>
      <td>ë¬´ëŒ€ì¥ì¹˜</td>
    </tr>
    <tr>
      <th>13</th>
      <td>id_000014</td>
      <td>Q</td>
      <td>86</td>
      <td>862</td>
      <td>ì˜ì›ì—ì„œ</td>
      <td>ì†Œì•„,ì²­ì†Œë…„ì„ëŒ€ìƒìœ¼ë¡œ</td>
      <td>ì§„ë£Œì„œë¹„ìŠ¤</td>
    </tr>
    <tr>
      <th>14</th>
      <td>id_000015</td>
      <td>M</td>
      <td>72</td>
      <td>729</td>
      <td>ì‚°ì—…ê³µë‹¨ì¡°ì„±ì‹œ ì˜ë¢°ë°›ì•„</td>
      <td>ì‚¬ë¬´ì‹¤ì—ì„œ</td>
      <td>ì¸¡ëŸ‰ï¼Œí† ëª©ì„¤ê³„</td>
    </tr>
    <tr>
      <th>15</th>
      <td>id_000016</td>
      <td>S</td>
      <td>96</td>
      <td>961</td>
      <td>íƒœêµ­ì „í†µë§ˆì‚¬ì§€ìƒµ</td>
      <td>ì¼ë°˜ê³ ê°ëŒ€ìƒ</td>
      <td>ë§ˆì‚¬ì§€</td>
    </tr>
    <tr>
      <th>16</th>
      <td>id_000017</td>
      <td>G</td>
      <td>46</td>
      <td>465</td>
      <td>ì‚¬ì—…ì¥ì—ì„œ</td>
      <td>ê´€ë ¨ì‚¬ìš©ìì—ê²Œ</td>
      <td>ì„ ë°•ì—”ì§„ë¶€í’ˆ</td>
    </tr>
    <tr>
      <th>17</th>
      <td>id_000018</td>
      <td>C</td>
      <td>10</td>
      <td>107</td>
      <td>ìƒê°€ì—ì„œ</td>
      <td>ì£¼ë¬¸ì— ì˜í•´</td>
      <td>ë°˜ì°¬,ë„ì‹œë½</td>
    </tr>
    <tr>
      <th>18</th>
      <td>id_000019</td>
      <td>G</td>
      <td>47</td>
      <td>476</td>
      <td>ë¬¸êµ¬ìš©í’ˆì—ì„œ</td>
      <td>ì¼ë°˜ì¸ëŒ€ìƒ</td>
      <td>ë¬¸êµ¬ë¥˜,ì‚¬ë¬´ìš©í’ˆ ì†Œë§¤</td>
    </tr>
    <tr>
      <th>19</th>
      <td>id_000020</td>
      <td>M</td>
      <td>72</td>
      <td>721</td>
      <td>ì—”ì§€ë‹ˆì–´ë§ë° ê±´ì¶•ê´€ë ¨</td>
      <td>ê³ ê° ìš”ì²­ì— ì˜í•´</td>
      <td>ë„ë©´ì„¤ê³„</td>
    </tr>
    <tr>
      <th>20</th>
      <td>id_000021</td>
      <td>C</td>
      <td>25</td>
      <td>259</td>
      <td>ìŠ¤í…Œì¸ë ˆìŠ¤ë©”íƒˆ</td>
      <td>ë„ê¸ˆ</td>
      <td>ìƒ¤ì›Œí•´ë“œì œì¡°</td>
    </tr>
    <tr>
      <th>21</th>
      <td>id_000022</td>
      <td>G</td>
      <td>46</td>
      <td>464</td>
      <td>ë§¤ë‹ˆíì–´ìš©ê¸°</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>22</th>
      <td>id_000023</td>
      <td>Q</td>
      <td>87</td>
      <td>872</td>
      <td>ìš”ì–‘ë³´í˜¸ì„¼í„°ì—ì„œ</td>
      <td>ê³ ê°ì— ì˜ë¢°ë¥¼ ë°›ì•„</td>
      <td>ìš”ì–‘ë³´í˜¸ì‚¬íŒŒê²¬</td>
    </tr>
    <tr>
      <th>23</th>
      <td>id_000024</td>
      <td>S</td>
      <td>96</td>
      <td>969</td>
      <td>ì í¬</td>
      <td>ì¼ë°˜ì¸</td>
      <td>ì„¸íƒì„œë¹„ìŠ¤</td>
    </tr>
    <tr>
      <th>24</th>
      <td>id_000025</td>
      <td>C</td>
      <td>32</td>
      <td>320</td>
      <td>ìš°ë“œì¹´ìš´í‹°ì—ì„œ</td>
      <td>ë‚˜ë¬´ë¥¼ì´ìš©í•´</td>
      <td>ê°€êµ¬ì£¼ë¬¸ì œì‘</td>
    </tr>
    <tr>
      <th>25</th>
      <td>id_000026</td>
      <td>S</td>
      <td>95</td>
      <td>951</td>
      <td>ì˜ì—…ì¥ì—ì„œ</td>
      <td>ê³ ê°ìš”êµ¬ë¡œ</td>
      <td>ì»´í“¨í„°.í•¸ë“œí°ìˆ˜ë¦¬</td>
    </tr>
    <tr>
      <th>26</th>
      <td>id_000027</td>
      <td>G</td>
      <td>47</td>
      <td>474</td>
      <td>êµ¬ë‘ê°€ê²Œì—ì„œ</td>
      <td>ì¼ë°˜ì¸ì„ ëŒ€ìƒìœ¼ë¡œ</td>
      <td>êµ¬ë‘ì†Œë§¤</td>
    </tr>
    <tr>
      <th>27</th>
      <td>id_000028</td>
      <td>L</td>
      <td>68</td>
      <td>682</td>
      <td>ì‚¬ì—…ì¥ì—ì„œ</td>
      <td>ê³ ê°ì˜ë¢°ë¡œ</td>
      <td>ì¤‘ê°œì„œë¹„ìŠ¤</td>
    </tr>
    <tr>
      <th>28</th>
      <td>id_000029</td>
      <td>L</td>
      <td>68</td>
      <td>682</td>
      <td>ë¶€ë™ì‚°</td>
      <td>ê³„ì•½ë°ì¤‘ê°œìˆ˜ìˆ˜ë£Œë°›ê³ </td>
      <td>ë¶€ë™ì‚°ê±°ë˜ì¤‘ê°œì„œë¹„ìŠ¤</td>
    </tr>
    <tr>
      <th>29</th>
      <td>id_000030</td>
      <td>R</td>
      <td>91</td>
      <td>912</td>
      <td>pcë°©ì—ì„œ</td>
      <td>ì¸í„°ë„·ì‹œì„¤ì„ ê°–ì¶”ê³ </td>
      <td>ì¸í„°ë„·ì„œë¹„ìŠ¤ì œê³µ</td>
    </tr>
    <tr>
      <th>30</th>
      <td>id_000031</td>
      <td>P</td>
      <td>85</td>
      <td>855</td>
      <td>ì‚¬ì—…ì¥ì—ì„œ</td>
      <td>ì–´ë¦°ì´ë“¤ì„ ëŒ€ìƒìœ¼ë¡œ</td>
      <td>ë°©ë¬¸êµìœ¡í•™ìŠµì§€</td>
    </tr>
    <tr>
      <th>31</th>
      <td>id_000032</td>
      <td>G</td>
      <td>46</td>
      <td>463</td>
      <td>ì‚°ì—…ì‚¬ìš©ìì—ê²Œ</td>
      <td>ì„ ë³„ì¥ì—ì„œ</td>
      <td>ë†ì‚°ë¬¼ë„ë§¤</td>
    </tr>
    <tr>
      <th>32</th>
      <td>id_000033</td>
      <td>I</td>
      <td>56</td>
      <td>561</td>
      <td>ìŒì‹ì ì—ì„œ</td>
      <td>ì ‘ê°ì‹œì„¤ì„ê°–ì¶”ê³ </td>
      <td>ë³´ìŒˆìŒì‹</td>
    </tr>
    <tr>
      <th>33</th>
      <td>id_000034</td>
      <td>C</td>
      <td>25</td>
      <td>251</td>
      <td>ìƒ¤ì‹œ,ë°©ë²”ì°½</td>
      <td>ì‚¬ì—…ì¥ì—ì„œ</td>
      <td>ì œì‘.ìš©ì ‘</td>
    </tr>
    <tr>
      <th>34</th>
      <td>id_000035</td>
      <td>S</td>
      <td>96</td>
      <td>961</td>
      <td>ì´ìš©ì›</td>
      <td>ë‚¨ì„±ë‘ë°œì„œë¹„ìŠ¤</td>
      <td>ì»¤íŠ¸,íŒŒë§ˆ</td>
    </tr>
    <tr>
      <th>35</th>
      <td>id_000036</td>
      <td>I</td>
      <td>56</td>
      <td>561</td>
      <td>ìŒì‹ì ì—ì„œ</td>
      <td>ì ‘ê°ì‹œì„¤ì„ê°—ì¶”ê³ </td>
      <td>ë¼ì§€ìˆ˜ìœ¡,ìˆœëŒ€ì •ì‹</td>
    </tr>
    <tr>
      <th>36</th>
      <td>id_000037</td>
      <td>G</td>
      <td>47</td>
      <td>478</td>
      <td>ê½ƒì‹œì¥ì—ì„œ</td>
      <td>ì¼ë°˜ì†Œë¹„ì ëŒ€ìƒìœ¼ë¡œ</td>
      <td>ê´€ì—½ ì†Œë§¤</td>
    </tr>
    <tr>
      <th>37</th>
      <td>id_000038</td>
      <td>J</td>
      <td>61</td>
      <td>612</td>
      <td>ì¹´ë“œì‚¬ë“±</td>
      <td>G/W ê¸°ìˆ </td>
      <td>SMS ì„œë¹„ìŠ¤</td>
    </tr>
    <tr>
      <th>38</th>
      <td>id_000039</td>
      <td>C</td>
      <td>13</td>
      <td>139</td>
      <td>PP.PET</td>
      <td>ì›ë£Œíˆ¬ì…, ì••ì¶œ</td>
      <td>ì›ì‚¬</td>
    </tr>
    <tr>
      <th>39</th>
      <td>id_000040</td>
      <td>Q</td>
      <td>86</td>
      <td>862</td>
      <td>ë‚´ê³¼ì˜ì›ì—ì„œ</td>
      <td>ì™¸ë˜í™˜ììœ„ì£¼ë¡œ</td>
      <td>ë‚´ê³¼</td>
    </tr>
    <tr>
      <th>40</th>
      <td>id_000041</td>
      <td>G</td>
      <td>47</td>
      <td>478</td>
      <td>ì•ˆê²½ì ‘ì—ì„œ</td>
      <td>ì¼ë°˜ê³ ê°ì„ ëŒ€ìƒìœ¼ë¡œ</td>
      <td>ì•ˆê²½,ë Œì¦ˆ</td>
    </tr>
    <tr>
      <th>41</th>
      <td>id_000042</td>
      <td>C</td>
      <td>18</td>
      <td>181</td>
      <td>ì¢…ì´ì¸ì‡„ë¬¼</td>
      <td>ì¬ë‹¨ê¸°ì‚¬ìš©</td>
      <td>ì¬ë‹¨(ì¸ì‡„ë¬¼)</td>
    </tr>
    <tr>
      <th>42</th>
      <td>id_000043</td>
      <td>G</td>
      <td>47</td>
      <td>477</td>
      <td>ê°€ì •ìš©ê°€ìŠ¤ì—°ë£Œë¥¼</td>
      <td>ê°€ì •ì†Œë¹„ìì—ê²Œ ì†Œë§¤</td>
      <td>ì¶©ì „ëœ ê°€ì •ìš©LPG70</td>
    </tr>
    <tr>
      <th>43</th>
      <td>id_000044</td>
      <td>I</td>
      <td>56</td>
      <td>562</td>
      <td>ë‹¤ë°©ì—ì„œ</td>
      <td>ì¼ë°˜ì¸ì„ ëŒ€ìƒìœ¼ë¡œ</td>
      <td>ì»¤í”¼</td>
    </tr>
    <tr>
      <th>44</th>
      <td>id_000045</td>
      <td>C</td>
      <td>33</td>
      <td>339</td>
      <td>ì œì¡°ì—…</td>
      <td>ì—…ìë¥¼ ëŒ€ìƒìœ¼ë¡œ</td>
      <td>ì¹«ì†”,ë¶€ëŸ¬ì‰¬</td>
    </tr>
    <tr>
      <th>45</th>
      <td>id_000046</td>
      <td>H</td>
      <td>52</td>
      <td>521</td>
      <td>ì‚¬ì—…ì¥ì—ì„œ</td>
      <td>ì² íŒŒì´í”„ë³´ê´€</td>
      <td>ì°½ê³ (ë¬¼ë¥˜)</td>
    </tr>
    <tr>
      <th>46</th>
      <td>id_000047</td>
      <td>I</td>
      <td>55</td>
      <td>551</td>
      <td>íœì…˜ì—ì„œ</td>
      <td>ê´€ê´‘ê°ì„ ëŒ€ìƒìœ¼ë¡œ</td>
      <td>ìˆ™ë°•ì„œë¹„ìŠ¤ì œê³µ</td>
    </tr>
    <tr>
      <th>47</th>
      <td>id_000048</td>
      <td>G</td>
      <td>46</td>
      <td>464</td>
      <td>ì†Œë§¤ì—…ìì—ê²ŒëŒ€ëŸ‰ìœ¼ë¡œë„ë§¤</td>
      <td>íŒë§¤ì¥ì—ì„œ</td>
      <td>ë¹„ëˆ„,ì„¸ì •ì œ,í™”ì¥í’ˆ,ì¹˜ì•½</td>
    </tr>
    <tr>
      <th>48</th>
      <td>id_000049</td>
      <td>M</td>
      <td>73</td>
      <td>732</td>
      <td>ì£¼ë¬¸ì— ì˜í•´</td>
      <td>ì¸í…Œë¦¬ì–´</td>
      <td>ë””ìì¸</td>
    </tr>
    <tr>
      <th>49</th>
      <td>id_000050</td>
      <td>G</td>
      <td>47</td>
      <td>471</td>
      <td>ë§¤ì¥ì—ì„œ</td>
      <td>ì¼ë°˜ì†Œë¹„ìì—ê²Œ</td>
      <td>ì‹ë£Œí’ˆì†Œë§¤1</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-ea3534e9-4f75-4ad2-aaea-09d6613cf35c')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-ea3534e9-4f75-4ad2-aaea-09d6613cf35c button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-ea3534e9-4f75-4ad2-aaea-09d6613cf35c');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python

```
