```python
#구글드라이브 연동
from google.colab import drive
drive.mount('/content/drive')
```

    Mounted at /content/drive
    


```python
# gpu 켜기
import torch
device = torch.device("cuda:0")
```


```python
# 저장 경로 미리 지정
path = '/content/drive/MyDrive/nlp_c/'
```

# **로그파일 연동 시키기**


```python
import logging

def make_logger(name=None):
    #1 logger instance를 만든다.
    logger = logging.getLogger(name)

    #2 logger의 level을 가장 낮은 수준인 DEBUG로 설정해둔다.
    logger.setLevel(logging.DEBUG)

    #3 formatter 지정
    formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    
    #4 handler instance 생성
    console = logging.StreamHandler()
    file_handler = logging.FileHandler(filename=path + "logs/correct_final.log",
                                       encoding = 'utf-8')
    
    #5 handler 별로 다른 level 설정
    console.setLevel(logging.INFO)
    file_handler.setLevel(logging.DEBUG)

    #6 handler 출력 format 지정
    console.setFormatter(formatter)
    file_handler.setFormatter(formatter)

    #7 logger에 handler 추가
    logger.addHandler(console)
    logger.addHandler(file_handler)

    return logger
```


```python
logger = make_logger()

logger.debug("debug logging")
logger.info("info logging")
logger.warning("warning logging")
logger.error("error logging")
logger.critical("critical logging")
```

    2022-04-12 17:21:52,192 - root - INFO - info logging
    2022-04-12 17:21:52,194 - root - WARNING - warning logging
    2022-04-12 17:21:52,195 - root - ERROR - error logging
    2022-04-12 17:21:52,198 - root - CRITICAL - critical logging
    

# **필요한 환경 다운 및 구축**

## 학습모델 패키지 다운 및 구축(KoBERT)


```python
#깃허브에서 KoBERT 파일 로드
!pip install ipywidgets  # for vscode
!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master
```

    Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (7.7.0)
    Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (3.6.0)
    Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.2.0)
    Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.5.0)
    Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (1.1.0)
    Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (0.2.0)
    Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (4.10.1)
    Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.1.1)
    Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.1.1)
    Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.5)
    Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)
    Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (1.0.18)
    Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)
    Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (2.6.1)
    Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (0.8.1)
    Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)
    Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (57.4.0)
    Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets) (4.9.2)
    Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets) (4.3.3)
    Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.10.0.2)
    Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (4.11.3)
    Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.1)
    Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (5.4.0)
    Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.4.0)
    Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.7.0)
    Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets) (0.2.5)
    Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets) (1.15.0)
    Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (5.3.1)
    Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)
    Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.11.3)
    Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.13.3)
    Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.6.1)
    Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (22.3.0)
    Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)
    Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.0)
    Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0.1)
    Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)
    Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.6.0)
    Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.1.0)
    Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)
    Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)
    Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.4)
    Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.3)
    Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)
    Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.7)
    Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master
      Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-t4wud7uv
      Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-t4wud7uv
    Collecting boto3
      Downloading boto3-1.21.38-py3-none-any.whl (132 kB)
    [K     |████████████████████████████████| 132 kB 4.1 MB/s 
    [?25hCollecting gluonnlp>=0.6.0
      Downloading gluonnlp-0.10.0.tar.gz (344 kB)
    [K     |████████████████████████████████| 344 kB 33.4 MB/s 
    [?25hCollecting mxnet>=1.4.0
      Downloading mxnet-1.9.0-py3-none-manylinux2014_x86_64.whl (47.3 MB)
    [K     |████████████████████████████████| 47.3 MB 1.1 MB/s 
    [?25hCollecting onnxruntime==1.8.0
      Downloading onnxruntime-1.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)
    [K     |████████████████████████████████| 4.5 MB 79.4 MB/s 
    [?25hCollecting sentencepiece>=0.1.6
      Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)
    [K     |████████████████████████████████| 1.2 MB 59.6 MB/s 
    [?25hRequirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (1.10.0+cu111)
    Collecting transformers>=4.8.1
      Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)
    [K     |████████████████████████████████| 4.0 MB 49.4 MB/s 
    [?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (3.17.3)
    Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (1.21.5)
    Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (2.0)
    Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (0.29.28)
    Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (21.3)
    Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet>=1.4.0->kobert==0.2.3) (2.23.0)
    Collecting graphviz<0.9.0,>=0.8.1
      Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)
    Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (1.24.3)
    Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (3.0.4)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (2021.10.8)
    Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (2.10)
    Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->kobert==0.2.3) (3.10.0.2)
    Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (3.6.0)
    Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (2019.12.20)
    Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (4.63.0)
    Collecting huggingface-hub<1.0,>=0.1.0
      Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)
    [K     |████████████████████████████████| 77 kB 8.6 MB/s 
    [?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (4.11.3)
    Collecting tokenizers!=0.11.3,<0.13,>=0.11.1
      Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)
    [K     |████████████████████████████████| 6.5 MB 56.7 MB/s 
    [?25hCollecting sacremoses
      Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)
    [K     |████████████████████████████████| 895 kB 81.6 MB/s 
    [?25hCollecting pyyaml>=5.1
      Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)
    [K     |████████████████████████████████| 596 kB 79.5 MB/s 
    [?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp>=0.6.0->kobert==0.2.3) (3.0.7)
    Collecting jmespath<2.0.0,>=0.7.1
      Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)
    Collecting botocore<1.25.0,>=1.24.38
      Downloading botocore-1.24.38-py3-none-any.whl (8.7 MB)
    [K     |████████████████████████████████| 8.7 MB 51.5 MB/s 
    [?25hCollecting s3transfer<0.6.0,>=0.5.0
      Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)
    [K     |████████████████████████████████| 79 kB 11.4 MB/s 
    [?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1
      Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)
    [K     |████████████████████████████████| 127 kB 91.6 MB/s 
    [?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.38->boto3->kobert==0.2.3) (2.8.2)
    Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.38->boto3->kobert==0.2.3) (1.15.0)
    Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.8.1->kobert==0.2.3) (3.7.0)
    Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.2.3) (7.1.2)
    Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.2.3) (1.1.0)
    Building wheels for collected packages: kobert, gluonnlp
      Building wheel for kobert (setup.py) ... [?25l[?25hdone
      Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15674 sha256=234c7d6793d34f714a8fb2c75ec7e2d7872a43a5a57b8babbcdaff80a8072180
      Stored in directory: /tmp/pip-ephem-wheel-cache-lwdz75rt/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0
      Building wheel for gluonnlp (setup.py) ... [?25l[?25hdone
      Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595739 sha256=063abdebf78d0703cc31721a8960e0949f2618e2401a0934306b55de35a73a33
      Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00
    Successfully built kobert gluonnlp
    Installing collected packages: urllib3, jmespath, pyyaml, botocore, tokenizers, sacremoses, s3transfer, huggingface-hub, graphviz, transformers, sentencepiece, onnxruntime, mxnet, gluonnlp, boto3, kobert
      Attempting uninstall: urllib3
        Found existing installation: urllib3 1.24.3
        Uninstalling urllib3-1.24.3:
          Successfully uninstalled urllib3-1.24.3
      Attempting uninstall: pyyaml
        Found existing installation: PyYAML 3.13
        Uninstalling PyYAML-3.13:
          Successfully uninstalled PyYAML-3.13
      Attempting uninstall: graphviz
        Found existing installation: graphviz 0.10.1
        Uninstalling graphviz-0.10.1:
          Successfully uninstalled graphviz-0.10.1
    [31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
    datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.[0m
    Successfully installed boto3-1.21.38 botocore-1.24.38 gluonnlp-0.10.0 graphviz-0.8.4 huggingface-hub-0.5.1 jmespath-1.0.0 kobert-0.2.3 mxnet-1.9.0 onnxruntime-1.8.0 pyyaml-6.0 s3transfer-0.5.2 sacremoses-0.0.49 sentencepiece-0.1.96 tokenizers-0.11.6 transformers-4.18.0 urllib3-1.25.11
    


```python
# 필요한 모듈 로딩
import pandas as pd
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gluonnlp as nlp
import numpy as np
from tqdm.notebook import tqdm
from tqdm import tqdm_notebook
```


```python
#kobert
from kobert.utils import get_tokenizer
from kobert.pytorch_kobert import get_pytorch_kobert_model

#transformers
from transformers import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup
```


```python
#BERT 모델, Vocabulary 불러오기
bertmodel, vocab = get_pytorch_kobert_model()
```

    /content/.cache/kobert_v1.zip[██████████████████████████████████████████████████]
    /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece[██████████████████████████████████████████████████]
    


```python

```

# **맞춤법 검사한 데이터 불러온 뒤 전처리(EDA 방법 포함)**


```python
# 맞춤법 검사 완료한 train 파일 불러오기
train_d = pd.read_csv(path + 'final_data/' + 'correct_train_fin.csv', encoding = 'utf-8-sig')
print(len(train_d))
```

    705530
    


```python
# index 변경 -> 아래 모델 생성에서 index 오류 발생하여 0부터 re-indexing
# predict 값 추출하고, 다시 원래 y 값으로 변환해주는 방식으로 해야할 듯
# 소분류 dictionary
y_dict = pd.DataFrame({'origin_y' : train_d['y'].unique()}).sort_values(by = 'origin_y')
y_dict['y'] = np.arange(0, len(y_dict))
y_dict = y_dict.astype('str')
s_dict0 = y_dict.set_index('origin_y').to_dict()['y'] # 처음 y값을 모델 train 을 위해 re-indexing
s_dict1 = y_dict.set_index('y').to_dict()['origin_y'] # 뒤에 예측값 다시 y 값으로 return 할 때 사용
```


```python
# kobert 모델 학습을 위해 reindexing 한 dictionary 저장 -> 후에 모델 예측값 도출 후, 기존 y값으로 되돌리기 위함
import pickle
with open(path+ 'final_data/' + 's_dictionary', 'wb') as f:
    pickle.dump(s_dict1, f)
```


```python
train_d['y_s'] = train_d['y'].astype('str')
train_d['label_s'] = train_d['y_s'].map(s_dict0)
```

## EDA 부분


```python
# text augmentation
# pip install -U nltk
import nltk; 
nltk.download('omw-1.4');
# nltk.download('wordnet') # 영문 버전
```

    [nltk_data] Downloading package omw-1.4 to /root/nltk_data...
    [nltk_data]   Unzipping corpora/omw-1.4.zip.
    


```python
# eda 폴더 생성
% cd /content/drive/MyDrive/nlp_c
# !git clone https://github.com/jasonwei20/eda_nlp
# !git clone https://github.com/catSirup/KorEDA
# eda는 eda_nlp/code 폴더에, wordnet.pickle 은 eda_nlp 폴더로 이동시키고, 진행
% cd eda_nlp/
# 추가적으로 augment.py 64번째 항에 eda -> EDA로 변경해야 실행됨
```

    /content/drive/MyDrive/nlp_c
    /content/drive/MyDrive/nlp_c/eda_nlp
    


```python
s_class_n = pd.DataFrame(train_d['y_s'].value_counts().sort_values())
# s_class_n.to_csv(path + 'testset_class.csv', index=False, encoding='EUC-KR')
s_class = s_class_n[s_class_n['y_s'] < 500].index.tolist()
len(s_class)
```




    85




```python
# n수가 부족한 class aug_num 차등으로 증강(적은 순서대로 20, 10 ,5) -> 상대적으로 부족한 클래스 데이터 비율이 더 높아지는 것을 조금이라도 방지하고자 함
s_class1 = s_class[:30]
s_class2 = s_class[30:60]
s_class3 = s_class[60:]
```


```python
few_d1 = train_d[train_d['y_s'].isin(s_class1)]
few_d2 = train_d[train_d['y_s'].isin(s_class2)]
few_d3 = train_d[train_d['y_s'].isin(s_class3)]
```


```python
# n이 100개 이하인 클래스 뽑아서 augmentation 가능한 파일 형태로 만들어주기
txt_aug_list = [str(a) + '\t' + str(b) for a, b in zip(few_d1['label_s'], few_d1['clean_done'])]
with open(path + 'final_data/' + 'text_aug_1.txt', 'w') as f:
  f.write('\n'.join(txt_aug_list) + '\n')

txt_aug_list = [str(a) + '\t' + str(b) for a, b in zip(few_d2['label_s'], few_d2['clean_done'])]
with open(path + 'final_data/' + 'text_aug_2.txt', 'w') as f:
  f.write('\n'.join(txt_aug_list) + '\n')

txt_aug_list = [str(a) + '\t' + str(b) for a, b in zip(few_d3['label_s'], few_d3['clean_done'])]
with open(path + 'final_data/' + 'text_aug_3.txt', 'w') as f:
  f.write('\n'.join(txt_aug_list) + '\n')

# input file 형식 -> txt 파일 내 한 행 당 label + \t + text 형태로 들어간 파일 
# SR: Synonym Replacement, 특정 단어를 유의어로 교체
# RI: Random Insertion, 임의의 단어를 삽입
# RS: Random Swap, 문장 내 임의의 두 단어의 위치를 바꿈
# RD: Random Deletion: 임의의 단어를 삭제
!python code/augment.py --input=/content/drive/MyDrive/nlp_c/final_data/text_aug_1.txt --output=/content/drive/MyDrive/nlp_c/final_data/test_aug_eda_1.txt --num_aug=20 --alpha_sr=0.1 --alpha_rd=0.2 --alpha_ri=0.1 --alpha_rs=0.0
!python code/augment.py --input=/content/drive/MyDrive/nlp_c/final_data/text_aug_2.txt --output=/content/drive/MyDrive/nlp_c/final_data/test_aug_eda_2.txt --num_aug=10 --alpha_sr=0.1 --alpha_rd=0.2 --alpha_ri=0.1 --alpha_rs=0.0
!python code/augment.py --input=/content/drive/MyDrive/nlp_c/final_data/text_aug_3.txt --output=/content/drive/MyDrive/nlp_c/final_data/test_aug_eda_3.txt --num_aug=5 --alpha_sr=0.1 --alpha_rd=0.2 --alpha_ri=0.1 --alpha_rs=0.0
```

    generated augmented sentences with eda for /content/drive/MyDrive/nlp_c/final_data/text_aug_1.txt to /content/drive/MyDrive/nlp_c/final_data/test_aug_eda_1.txt with num_aug=20
    generated augmented sentences with eda for /content/drive/MyDrive/nlp_c/final_data/text_aug_2.txt to /content/drive/MyDrive/nlp_c/final_data/test_aug_eda_2.txt with num_aug=10
    generated augmented sentences with eda for /content/drive/MyDrive/nlp_c/final_data/text_aug_3.txt to /content/drive/MyDrive/nlp_c/final_data/test_aug_eda_3.txt with num_aug=5
    


```python
# augmentation 완료한 데이터 불러와서 기존데이터셋에 붙여주기(augmentation 대상 데이터는 삭제)
with open('/content/drive/MyDrive/nlp_c/final_data/test_aug_eda_1.txt', "r") as file:
  strings = file.readlines()
aug_d1 = pd.DataFrame([x.split('\n')[0].split('\t') for x in strings])
aug_d1.columns = ['label_s', 'clean_done']

with open('/content/drive/MyDrive/nlp_c/final_data/test_aug_eda_2.txt', "r") as file:
  strings = file.readlines()
aug_d2 = pd.DataFrame([x.split('\n')[0].split('\t') for x in strings])
aug_d2.columns = ['label_s', 'clean_done']

with open('/content/drive/MyDrive/nlp_c/final_data/test_aug_eda_3.txt', "r") as file:
  strings = file.readlines()
aug_d3 = pd.DataFrame([x.split('\n')[0].split('\t') for x in strings])
aug_d3.columns = ['label_s', 'clean_done']

train_d = train_d[train_d['y_s'].isin(s_class)==False]
sample_d = pd.concat([train_d[['label_s', 'clean_done']], aug_d1, aug_d2, aug_d3], axis = 0, ignore_index = True)
```


```python
sample_d['len'] = sample_d['clean_done'].astype(str).apply(len)
```


```python
i = 798000
sample_d[i:i+50]
```





  <div id="df-716e8000-ebb4-4c61-98a6-94ddd5f329e7">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label_s</th>
      <th>clean_done</th>
      <th>len</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>798000</th>
      <td>139</td>
      <td>당진화력 예인선 현장에서 내 기타 해상 운수업</td>
      <td>25</td>
    </tr>
    <tr>
      <th>798001</th>
      <td>139</td>
      <td>당진화력 내 현장에서 예인선 기타 해상 운수업</td>
      <td>25</td>
    </tr>
    <tr>
      <th>798002</th>
      <td>103</td>
      <td>각종 건설현장 폐기물을 중간처리</td>
      <td>17</td>
    </tr>
    <tr>
      <th>798003</th>
      <td>103</td>
      <td>각종 건설현장 폐기물을 중간처리</td>
      <td>17</td>
    </tr>
    <tr>
      <th>798004</th>
      <td>103</td>
      <td>각종 건설현장 폐기물을 중간처리</td>
      <td>17</td>
    </tr>
    <tr>
      <th>798005</th>
      <td>103</td>
      <td>각종 건설현장 폐기물을</td>
      <td>12</td>
    </tr>
    <tr>
      <th>798006</th>
      <td>103</td>
      <td>건설현장 각종 폐기물을 중간처리</td>
      <td>17</td>
    </tr>
    <tr>
      <th>798007</th>
      <td>103</td>
      <td>각종 건설현장 폐기물을 중간처리</td>
      <td>17</td>
    </tr>
    <tr>
      <th>798008</th>
      <td>196</td>
      <td>환경 관리사업소에서 하수 및 수행하여 폐기물 환경보존 행정</td>
      <td>32</td>
    </tr>
    <tr>
      <th>798009</th>
      <td>196</td>
      <td>환경 관리사업소에서 하수 폐기물 및 위생처리를 수행하여 폐기물 환경보존 경</td>
      <td>41</td>
    </tr>
    <tr>
      <th>798010</th>
      <td>196</td>
      <td>환경 관리사업소에서 하수 폐기물 위생처리를 및 수행하여 폐기물 환경보존 행정</td>
      <td>42</td>
    </tr>
    <tr>
      <th>798011</th>
      <td>196</td>
      <td>환경 행 관리사업소에서 하수 폐기물 및 위생처리를 수행하여 폐기물 환경보존 행정</td>
      <td>44</td>
    </tr>
    <tr>
      <th>798012</th>
      <td>196</td>
      <td>환경 관리사업소에서 하수 폐기물 및 위생처리를 수행하여 폐기물 행 환경보존 행정</td>
      <td>44</td>
    </tr>
    <tr>
      <th>798013</th>
      <td>196</td>
      <td>환경 관리사업소에서 하수 폐기물 및 위생처리를 수행하여 폐기물 환경보존 행정</td>
      <td>42</td>
    </tr>
    <tr>
      <th>798014</th>
      <td>103</td>
      <td>건설폐기물을 받아 분리 건설폐기물처리 처리</td>
      <td>23</td>
    </tr>
    <tr>
      <th>798015</th>
      <td>103</td>
      <td>건설폐기물을 받아 분리 처리</td>
      <td>15</td>
    </tr>
    <tr>
      <th>798016</th>
      <td>103</td>
      <td>건설폐기물을 분 받아 분리 건설폐기물처리 처리</td>
      <td>25</td>
    </tr>
    <tr>
      <th>798017</th>
      <td>103</td>
      <td>건설폐기물을 받아 리 건설폐기물처리 처리</td>
      <td>22</td>
    </tr>
    <tr>
      <th>798018</th>
      <td>103</td>
      <td>건설폐기물을 받아 분리 처리 건설폐기물처리</td>
      <td>23</td>
    </tr>
    <tr>
      <th>798019</th>
      <td>103</td>
      <td>건설폐기물을 받아 분리 건설폐기물처리 처리</td>
      <td>23</td>
    </tr>
    <tr>
      <th>798020</th>
      <td>83</td>
      <td>사업장에서 금속으로 자동차 레 제조</td>
      <td>19</td>
    </tr>
    <tr>
      <th>798021</th>
      <td>83</td>
      <td>사업장에서 금속으로 자동차 러 제조</td>
      <td>19</td>
    </tr>
    <tr>
      <th>798022</th>
      <td>83</td>
      <td>사업장에서 자 금속으로 자동차 트레일러 제조</td>
      <td>24</td>
    </tr>
    <tr>
      <th>798023</th>
      <td>83</td>
      <td>사업장에서 금속으로 자동차 트레일러 제조</td>
      <td>22</td>
    </tr>
    <tr>
      <th>798024</th>
      <td>83</td>
      <td>트레일러 금속으로 자동차 사업장에서 제조</td>
      <td>22</td>
    </tr>
    <tr>
      <th>798025</th>
      <td>83</td>
      <td>사업장에서 금속으로 자동차 트레일러 제조</td>
      <td>22</td>
    </tr>
    <tr>
      <th>798026</th>
      <td>116</td>
      <td>점포에서 소비자를 대상으로 토 소매</td>
      <td>19</td>
    </tr>
    <tr>
      <th>798027</th>
      <td>116</td>
      <td>점포에서 소비자를 소매 오토바이 대상으로</td>
      <td>22</td>
    </tr>
    <tr>
      <th>798028</th>
      <td>116</td>
      <td>점포에서 소비자를 대상으로 오토바이 소매</td>
      <td>22</td>
    </tr>
    <tr>
      <th>798029</th>
      <td>116</td>
      <td>점포에서 소비자를 오 대상으로 오토바이 소매</td>
      <td>24</td>
    </tr>
    <tr>
      <th>798030</th>
      <td>116</td>
      <td>점포에서 대상으로 소비자를 오토바이 소매</td>
      <td>22</td>
    </tr>
    <tr>
      <th>798031</th>
      <td>116</td>
      <td>점포에서 소비자를 대상으로 오토바이 소매</td>
      <td>22</td>
    </tr>
    <tr>
      <th>798032</th>
      <td>44</td>
      <td>요소 황산 반응 건 습식</td>
      <td>13</td>
    </tr>
    <tr>
      <th>798033</th>
      <td>44</td>
      <td>요소 건 황산 반응 건조 습식</td>
      <td>16</td>
    </tr>
    <tr>
      <th>798034</th>
      <td>44</td>
      <td>황산 반응 건조 습식</td>
      <td>11</td>
    </tr>
    <tr>
      <th>798035</th>
      <td>44</td>
      <td>요소 건조 반응 황산 습식</td>
      <td>14</td>
    </tr>
    <tr>
      <th>798036</th>
      <td>44</td>
      <td>황산 요소 반응 건조 습식</td>
      <td>14</td>
    </tr>
    <tr>
      <th>798037</th>
      <td>44</td>
      <td>요소  황산 반응  건조 습식</td>
      <td>18</td>
    </tr>
    <tr>
      <th>798038</th>
      <td>23</td>
      <td>Nylon Spandex 연사 실꼬임 스판덱스가</td>
      <td>26</td>
    </tr>
    <tr>
      <th>798039</th>
      <td>23</td>
      <td>Nylon 연사 Spandex 실꼬임 스판덱스가</td>
      <td>26</td>
    </tr>
    <tr>
      <th>798040</th>
      <td>23</td>
      <td>Nylon Spandex 실꼬임 스판덱스가</td>
      <td>23</td>
    </tr>
    <tr>
      <th>798041</th>
      <td>23</td>
      <td>Nylon Spandex 연사 실꼬임 스판덱스가</td>
      <td>26</td>
    </tr>
    <tr>
      <th>798042</th>
      <td>23</td>
      <td>Nylon Spandex 스판덱스가</td>
      <td>19</td>
    </tr>
    <tr>
      <th>798043</th>
      <td>23</td>
      <td>Nylon Spandex  연사 실꼬임  스판덱스가</td>
      <td>28</td>
    </tr>
    <tr>
      <th>798044</th>
      <td>46</td>
      <td>요소 황산철 유산철 원료 자 계량</td>
      <td>18</td>
    </tr>
    <tr>
      <th>798045</th>
      <td>46</td>
      <td>요소 황산철 원료 투입 계량</td>
      <td>15</td>
    </tr>
    <tr>
      <th>798046</th>
      <td>46</td>
      <td>요소 황산철 유산철 원료 투입 원 계량</td>
      <td>21</td>
    </tr>
    <tr>
      <th>798047</th>
      <td>46</td>
      <td>황산철 유산철 원료 투입 계량</td>
      <td>16</td>
    </tr>
    <tr>
      <th>798048</th>
      <td>46</td>
      <td>요소 황산철 유산철 투입 원료 계량</td>
      <td>19</td>
    </tr>
    <tr>
      <th>798049</th>
      <td>46</td>
      <td>요소  황산철 유산철 원료 투입  계량</td>
      <td>22</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-716e8000-ebb4-4c61-98a6-94ddd5f329e7')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-716e8000-ebb4-4c61-98a6-94ddd5f329e7 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-716e8000-ebb4-4c61-98a6-94ddd5f329e7');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python

```

# **처리한 데이터 kobert 모델에 학습**


```python
# train & test set 나누기
from sklearn.model_selection import train_test_split
# dataset_train, dataset_test = train_test_split(train_d, test_size=0.2, shuffle=True, random_state=30)
# dataset_test.to_csv('/content/drive/MyDrive/nlp_c/testset.csv', index=False, encoding = 'EUC-KR')

# stratify 를 target으로 지정해 비율을 맞춤으로써, 성능향상 가능
# but, 현재 target 변수 class 비율의 불균형으로 오류 발생(1,2 개짜리 class 다수 존재)
dataset_train, dataset_test, y_train, y_test = train_test_split(sample_d['clean_done'],
                               sample_d['label_s'], random_state=132, stratify=sample_d['label_s']) 
# 모델 검증용 미리 뽑아놓기
dataset_test.to_csv(path + 'final_data/' + 'testset.csv', index=False, encoding = 'utf-8-sig')
y_test.to_csv(path + 'final_data/' + 'testset_y.csv', index=False, encoding = 'utf-8-sig')
```


```python
dataset_train = [[str(a), str(b)] for a, b in zip(dataset_train, y_train)]
dataset_test = [[str(a), str(b)] for a, b in zip(dataset_test, y_test)]
```


```python
print(len(dataset_train))
print(len(dataset_test))
```

    614287
    204763
    


```python
print(dataset_train[:10])
print(dataset_test[:10])
```

    [['사업장에서 의뢰를 받아 철근 및 콘크리트 공사업', '108'], ['실내장식 집에서 벽지 장판 등을 이용하여 벽지 장판 판매 시공', '111'], ['두리 수산에서 일반 소비자에게 소매 각종 생선 팬 맥', '126'], ['내과 외래환자 내과 진료', '208'], ['부동산 중개소 중개 및 계약을 체결 수수료 받음 부동산 중개 서비스', '169'], ['건축 설계 및 관련 서비스업  건축설계', '178'], ['부동산 중개소에서 계약 및 중개에 의해 수수료 받음 부동산 거래 중개서비스', '169'], ['커피와 음료 판매  ', '147'], ['솜 원료를 중합  방사 ', '48'], ['사업장에서 일반인 방문객을 통하여 명상수련 서비스', '205']]
    [['신발가게에서 일반 소비자에게 소매 남녀 정장화', '128'], ['철 입고  표면가공 ', '63'], ['상가 관리사무소에서 상가 입주민을 위해 상가 관리 서비스', '169'], ['정육점에서 소비자를 대상으로 소고기 돼지고기', '126'], ['사무실에서 의뢰를 받아 세무대행', '173'], ['강습짱에서 일반 고객 대상으로 필라테스 강습', '205'], ['모터  감속기 조립  도장 ', '81'], ['산업 사용자에게 증기', '99'], ['모텔 고객 숙박 서비스', '145'], ['정육점에서 일반 소비자에게 소고기  돼지고기 판매', '147']]
    


```python
# KoBERT 입력 데이터로 만들기
# BERT 모델에 들어가기 위한 dataset을 만들어주는 클래스
class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,
                 pad, pair):
        transform = nlp.data.BERTSentenceTransform(
            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)

        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i], ))

    def __len__(self):
        return (len(self.labels))
```


```python

```


```python
# Setting parameters
# 이건 나중에 최적화 값 찾아봐야할 듯
max_len = 64
batch_size = 128
warmup_ratio = 0.1
num_epochs = 5
max_grad_norm = 1
log_interval = 200
learning_rate = 5e-5 # 0.0001 # 

# 토큰화 실행
tokenizer = get_tokenizer()
tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)
```

    using cached model. /content/drive/MyDrive/nlp_c/eda_nlp/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece
    


```python
data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, pad = True, pair = False)
data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, pad = True, pair = False)
```


```python
sentence = dataset_train[30][0] 
print(sentence)
print(tok(sentence))

# 토큰화 패딩 처리 후 결과값 
print(data_train[30])
```

    음식점에서 객석을 갖추고 한식 판매  
    ['▁음식', '점', '에서', '▁', '객', '석', '을', '▁갖추', '고', '▁한', '식', '▁판매']
    (array([   2, 3609, 7220, 6903,  517, 5370, 6557, 7088,  827, 5439, 4955,
           6730, 4809,    3,    1,    1,    1,    1,    1,    1,    1,    1,
              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
              1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
              1,    1,    1,    1,    1,    1,    1,    1,    1], dtype=int32), array(14, dtype=int32), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
          dtype=int32), 147)
    


```python
# torch 형식의 dataset 생성
# num_worker 은 gpu 활정화 정도, 5로 하니 오히려 과부화가 걸려 4로 조정
train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=4)
test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=4)
```


```python
# 클래스 수 조정
print(len(y_dict))
```

    225
    


```python
# KoBERT 학습모델 만들기
class BERTClassifier(nn.Module):
    def __init__(self,
                 bert,
                 hidden_size = 768,
                 num_classes=len(y_dict),   ##클래스 수 조정해줘야함##
                 dr_rate=None,
                 params=None):
        super(BERTClassifier, self).__init__()
        self.bert = bert
        self.dr_rate = dr_rate
                 
        self.classifier = nn.Linear(hidden_size , num_classes)
        if dr_rate:
            self.dropout = nn.Dropout(p=dr_rate)
    
    def gen_attention_mask(self, token_ids, valid_length):
        attention_mask = torch.zeros_like(token_ids)
        for i, v in enumerate(valid_length):
            attention_mask[i][:v] = 1
        return attention_mask.float()

    def forward(self, token_ids, valid_length, segment_ids):
        attention_mask = self.gen_attention_mask(token_ids, valid_length)
        
        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))
        if self.dr_rate:
            out = self.dropout(pooler)
        return self.classifier(out)
```


```python
# GPU 실행 오류 나면 사용
# import os
# os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
# os.environ["CUDA_VISIBLE_DEVICES"] = "0"
```


```python
model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)
```


```python
# Prepare optimizer and schedule (linear warmup and decay)
no_decay = ['bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
]
```


```python
optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)
loss_fn = nn.CrossEntropyLoss()
```

    /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
      FutureWarning,
    


```python
t_total = len(train_dataloader) * num_epochs
warmup_step = int(t_total * warmup_ratio)
```


```python
scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)
```


```python
def calc_accuracy(X,Y):
    max_vals, max_indices = torch.max(X, 1)
    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]
    return train_acc
```


```python
# pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html
```


```python
highest_acc = 0
patience = 0

# 최종 모델 학습시키기
for e in range(num_epochs):
    train_acc = 0.0
    test_acc = 0.0
    model.train()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):
        optimizer.zero_grad()
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)
        loss = loss_fn(out, label)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
        scheduler.step()  # Update learning rate schedule
        train_acc += calc_accuracy(out, label)
        if batch_id % log_interval == 0:
            print("epoch {} batch id {} loss {} train acc {}".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))
    print("epoch {} train acc {}".format(e+1, train_acc / (batch_id+1)))
    
    model.eval()

    for test_batch_id, (test_token_ids, test_valid_length, test_segment_ids, test_label) in enumerate(tqdm_notebook(test_dataloader)):
        test_token_ids = test_token_ids.long().to(device)
        test_segment_ids = test_segment_ids.long().to(device)
        test_valid_length= test_valid_length
        test_label = test_label.long().to(device)
        test_out = model(token_ids, valid_length, segment_ids)
        test_loss = loss_fn(out, label)
        test_acc += calc_accuracy(out, label)
    print("epoch {} test acc {}".format(e+1, test_acc / (test_batch_id+1)))

    if test_acc > highest_acc:
        torch.save({
            'epoch': e,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': test_loss,
            }, path + 'final_data/' + 'correct_model_fin.pt')
        patience = 0
    else:
        print("test acc did not improved. best:{} current:{}".format(highest_acc, test_acc))
        patience += 1
        if patience > 5:
            break
    print('current patience: {}'.format(patience))
    print("************************************************************************************")
```

    /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0
    Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`
      if __name__ == '__main__':
    


      0%|          | 0/4800 [00:00<?, ?it/s]


    epoch 1 batch id 1 loss 5.470526218414307 train acc 0.0
    epoch 1 batch id 201 loss 5.183148384094238 train acc 0.04287157960199005
    epoch 1 batch id 401 loss 4.4437031745910645 train acc 0.08350218204488778
    epoch 1 batch id 601 loss 3.7217705249786377 train acc 0.13737520798668884
    epoch 1 batch id 801 loss 3.2203402519226074 train acc 0.19774110486891386
    epoch 1 batch id 1001 loss 2.5548582077026367 train acc 0.2525131118881119
    epoch 1 batch id 1201 loss 2.060025215148926 train acc 0.3013049021648626
    epoch 1 batch id 1401 loss 1.7654931545257568 train acc 0.3442184154175589
    epoch 1 batch id 1601 loss 1.7790783643722534 train acc 0.3823538023110556
    epoch 1 batch id 1801 loss 1.3805230855941772 train acc 0.415585091615769
    epoch 1 batch id 2001 loss 1.0565495491027832 train acc 0.445050912043978
    epoch 1 batch id 2201 loss 1.0683343410491943 train acc 0.4712275670149932
    epoch 1 batch id 2401 loss 1.3318103551864624 train acc 0.49479708975426906
    epoch 1 batch id 2601 loss 0.9846600294113159 train acc 0.5164059496347558
    epoch 1 batch id 2801 loss 0.9582300782203674 train acc 0.5354588762941807
    epoch 1 batch id 3001 loss 0.7929444313049316 train acc 0.5528860171609463
    epoch 1 batch id 3201 loss 0.6908536553382874 train acc 0.5685332708528584
    epoch 1 batch id 3401 loss 0.7846947312355042 train acc 0.5832521684798588
    epoch 1 batch id 3601 loss 0.6151464581489563 train acc 0.5966419918078312
    epoch 1 batch id 3801 loss 0.6870463490486145 train acc 0.6090009208103131
    epoch 1 batch id 4001 loss 0.516919732093811 train acc 0.6200266339665084
    epoch 1 batch id 4201 loss 0.8247902393341064 train acc 0.6302535854558439
    epoch 1 batch id 4401 loss 0.7708945870399475 train acc 0.6398030987275619
    epoch 1 batch id 4601 loss 0.7333303689956665 train acc 0.648410331993045
    epoch 1 train acc 0.6567227647569445
    

    /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0
    Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`
    


      0%|          | 0/1600 [00:00<?, ?it/s]


    epoch 1 test acc 0.9333333333333352
    current patience: 0
    ************************************************************************************
    


      0%|          | 0/4800 [00:00<?, ?it/s]


    epoch 2 batch id 1 loss 0.6638566255569458 train acc 0.8359375
    epoch 2 batch id 201 loss 0.6112797260284424 train acc 0.8494247512437811
    epoch 2 batch id 401 loss 0.7496534585952759 train acc 0.8484842581047382
    epoch 2 batch id 601 loss 0.6063464879989624 train acc 0.8506395590682196
    epoch 2 batch id 801 loss 0.795397937297821 train acc 0.8525963639200999
    epoch 2 batch id 1001 loss 0.429190456867218 train acc 0.853279532967033
    epoch 2 batch id 1201 loss 0.6381816267967224 train acc 0.854001873438801
    epoch 2 batch id 1401 loss 0.4390967786312103 train acc 0.855638829407566
    epoch 2 batch id 1601 loss 0.5170191526412964 train acc 0.8566667317301686
    epoch 2 batch id 1801 loss 0.35547658801078796 train acc 0.8579348278734037
    epoch 2 batch id 2001 loss 0.3872775733470917 train acc 0.8594374687656172
    epoch 2 batch id 2201 loss 0.48807698488235474 train acc 0.8607344672875965
    epoch 2 batch id 2401 loss 0.6643775105476379 train acc 0.8618381663890046
    epoch 2 batch id 2601 loss 0.47221654653549194 train acc 0.8632617262591311
    epoch 2 batch id 2801 loss 0.43773651123046875 train acc 0.8640468805783649
    epoch 2 batch id 3001 loss 0.3916258215904236 train acc 0.8651985796401199
    epoch 2 batch id 3201 loss 0.4372307360172272 train acc 0.866157548422368
    epoch 2 batch id 3401 loss 0.4276728928089142 train acc 0.8671002094972067
    epoch 2 batch id 3601 loss 0.3759610652923584 train acc 0.8680531449597334
    epoch 2 batch id 3801 loss 0.5400181412696838 train acc 0.8691318896343068
    epoch 2 batch id 4001 loss 0.2910200357437134 train acc 0.8699446232191952
    epoch 2 batch id 4201 loss 0.6544965505599976 train acc 0.8706911300880743
    epoch 2 batch id 4401 loss 0.5287626385688782 train acc 0.8716183111792775
    epoch 2 batch id 4601 loss 0.5366830825805664 train acc 0.872120191262769
    epoch 2 train acc 0.8730557725694444
    


      0%|          | 0/1600 [00:00<?, ?it/s]


    epoch 2 test acc 0.9333333333333352
    current patience: 0
    ************************************************************************************
    


      0%|          | 0/4800 [00:00<?, ?it/s]


    epoch 3 batch id 1 loss 0.47743237018585205 train acc 0.859375
    epoch 3 batch id 201 loss 0.4529868960380554 train acc 0.8904306592039801
    epoch 3 batch id 401 loss 0.46464189887046814 train acc 0.8910536159600998
    epoch 3 batch id 601 loss 0.44063830375671387 train acc 0.8932378327787022
    epoch 3 batch id 801 loss 0.4417380094528198 train acc 0.8939509207240949
    epoch 3 batch id 1001 loss 0.32520782947540283 train acc 0.8938951673326674
    epoch 3 batch id 1201 loss 0.4275073707103729 train acc 0.8943263426311407
    epoch 3 batch id 1401 loss 0.3466779887676239 train acc 0.895130710206995
    epoch 3 batch id 1601 loss 0.3258431553840637 train acc 0.8953974078700812
    epoch 3 batch id 1801 loss 0.339708536863327 train acc 0.8959171987784564
    epoch 3 batch id 2001 loss 0.36911335587501526 train acc 0.8968250249875063
    epoch 3 batch id 2201 loss 0.38383665680885315 train acc 0.8975039754656974
    epoch 3 batch id 2401 loss 0.5033802390098572 train acc 0.898001483756768
    epoch 3 batch id 2601 loss 0.3143329322338104 train acc 0.8987919309880815
    epoch 3 batch id 2801 loss 0.30946600437164307 train acc 0.8991515307033202
    epoch 3 batch id 3001 loss 0.2630763053894043 train acc 0.8998667110963012
    epoch 3 batch id 3201 loss 0.36716964840888977 train acc 0.900250898156826
    epoch 3 batch id 3401 loss 0.3716908097267151 train acc 0.9007713723904733
    epoch 3 batch id 3601 loss 0.2624480426311493 train acc 0.9013360004165509
    epoch 3 batch id 3801 loss 0.3792986571788788 train acc 0.9020056399631676
    epoch 3 batch id 4001 loss 0.23668766021728516 train acc 0.9024404055236191
    epoch 3 batch id 4201 loss 0.5264573097229004 train acc 0.9028598101642467
    epoch 3 batch id 4401 loss 0.3679966628551483 train acc 0.903464766530334
    epoch 3 batch id 4601 loss 0.32983699440956116 train acc 0.9037607313627473
    epoch 3 train acc 0.9043473307291666
    


      0%|          | 0/1600 [00:00<?, ?it/s]


    epoch 3 test acc 1.0
    current patience: 0
    ************************************************************************************
    


      0%|          | 0/4800 [00:00<?, ?it/s]


    epoch 4 batch id 1 loss 0.344111829996109 train acc 0.8828125
    epoch 4 batch id 201 loss 0.36581531167030334 train acc 0.915967039800995
    epoch 4 batch id 401 loss 0.38386818766593933 train acc 0.9154652431421446
    epoch 4 batch id 601 loss 0.38915520906448364 train acc 0.9167013311148087
    epoch 4 batch id 801 loss 0.38615915179252625 train acc 0.9174274344569289
    epoch 4 batch id 1001 loss 0.2394852340221405 train acc 0.917239010989011
    epoch 4 batch id 1201 loss 0.2488376498222351 train acc 0.9173865528726062
    epoch 4 batch id 1401 loss 0.22874371707439423 train acc 0.9179101980728052
    epoch 4 batch id 1601 loss 0.2878819704055786 train acc 0.9182200577763897
    epoch 4 batch id 1801 loss 0.21825024485588074 train acc 0.9187777623542477
    epoch 4 batch id 2001 loss 0.2466854304075241 train acc 0.9195792728635682
    epoch 4 batch id 2201 loss 0.26348188519477844 train acc 0.9201250851885506
    epoch 4 batch id 2401 loss 0.4421859681606293 train acc 0.9205181434818825
    epoch 4 batch id 2601 loss 0.23925673961639404 train acc 0.9212292147251058
    epoch 4 batch id 2801 loss 0.24297061562538147 train acc 0.9213338986076401
    epoch 4 batch id 3001 loss 0.17798040807247162 train acc 0.9218567769076974
    epoch 4 batch id 3201 loss 0.260630339384079 train acc 0.9221703178694158
    epoch 4 batch id 3401 loss 0.26668980717658997 train acc 0.9225090047044987
    epoch 4 batch id 3601 loss 0.1432698667049408 train acc 0.9228729866703693
    epoch 4 batch id 3801 loss 0.27312996983528137 train acc 0.9232644369902657
    epoch 4 batch id 4001 loss 0.21531520783901215 train acc 0.9235445044988753
    epoch 4 batch id 4201 loss 0.4101676642894745 train acc 0.9238406778148059
    epoch 4 batch id 4401 loss 0.27769315242767334 train acc 0.9242324187684617
    epoch 4 batch id 4601 loss 0.26794159412384033 train acc 0.9242997446207346
    epoch 4 train acc 0.9246866319444444
    


      0%|          | 0/1600 [00:00<?, ?it/s]


    epoch 4 test acc 0.9333333333333352
    current patience: 0
    ************************************************************************************
    


      0%|          | 0/4800 [00:00<?, ?it/s]


    epoch 5 batch id 1 loss 0.2737545073032379 train acc 0.9140625
    epoch 5 batch id 201 loss 0.2553758919239044 train acc 0.9324860074626866
    epoch 5 batch id 401 loss 0.2817900776863098 train acc 0.9322007481296758
    epoch 5 batch id 601 loss 0.2567768692970276 train acc 0.9328462978369384
    epoch 5 batch id 801 loss 0.2468549758195877 train acc 0.9336278870162297
    epoch 5 batch id 1001 loss 0.22222311794757843 train acc 0.9333791208791209
    epoch 5 batch id 1201 loss 0.25492382049560547 train acc 0.9332717527060783
    epoch 5 batch id 1401 loss 0.15025579929351807 train acc 0.9334515524625268
    epoch 5 batch id 1601 loss 0.22277267277240753 train acc 0.933493714865709
    epoch 5 batch id 1801 loss 0.19275978207588196 train acc 0.9338171501943365
    epoch 5 batch id 2001 loss 0.22030514478683472 train acc 0.9345483508245878
    epoch 5 batch id 2201 loss 0.21622973680496216 train acc 0.9348627044525216
    epoch 5 batch id 2401 loss 0.3563814163208008 train acc 0.9350205643481883
    epoch 5 batch id 2601 loss 0.2274296134710312 train acc 0.9353794213763937
    epoch 5 batch id 2801 loss 0.2206992506980896 train acc 0.9353300160656908
    epoch 5 batch id 3001 loss 0.12332244217395782 train acc 0.9355943852049317
    epoch 5 batch id 3201 loss 0.22073575854301453 train acc 0.9356060606060606
    epoch 5 batch id 3401 loss 0.20686203241348267 train acc 0.9357036533372537
    epoch 5 batch id 3601 loss 0.1583988219499588 train acc 0.9359314252985282
    epoch 5 batch id 3801 loss 0.2532764673233032 train acc 0.9360961753485925
    epoch 5 batch id 4001 loss 0.15411587059497833 train acc 0.9361585384903774
    epoch 5 batch id 4201 loss 0.30796435475349426 train acc 0.9362651749583433
    epoch 5 batch id 4401 loss 0.23673692345619202 train acc 0.9364260253351511
    epoch 5 batch id 4601 loss 0.23493117094039917 train acc 0.9363640377091936
    epoch 5 train acc 0.9365494791666666
    


      0%|          | 0/1600 [00:00<?, ?it/s]


    epoch 5 test acc 1.0
    current patience: 0
    ************************************************************************************
    


```python
# # 학습한 모델 pickle 형태로 저장

# import pickle
# # path = '/content/drive/MyDrive/nlp_c/'

# with open(path+'model_trial_fin_noclean.pickle', 'wb') as f:
#     pickle.dump(model, f)
```


```python

```

# **코드북으로 제출형식 만들기**

## 한국표준산업분류(10차)_국문 자료 이용해서 코드 북 만들기


```python
import numpy as np 
import pandas as pd
```


```python
path = '/content/drive/MyDrive/nlp_c/'
code_book = pd.read_excel(path + '한국표준산업분류(10차)_국문.xlsx', header = 1)
```


```python
code_book = code_book.dropna(subset = ['소분류(232)'])
code_book[:10]
```





  <div id="df-c488b6e9-5c18-4599-bd39-cd5701d74f41">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>대분류(21)</th>
      <th>Unnamed: 1</th>
      <th>중분류(77)</th>
      <th>Unnamed: 3</th>
      <th>소분류(232)</th>
      <th>Unnamed: 5</th>
      <th>세분류(495)</th>
      <th>Unnamed: 7</th>
      <th>세세분류(1,196)</th>
      <th>Unnamed: 9</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>코드</td>
      <td>항목명</td>
      <td>코드</td>
      <td>항목명</td>
      <td>코드</td>
      <td>항목명</td>
      <td>코드</td>
      <td>항목명</td>
      <td>코드</td>
      <td>항목명</td>
    </tr>
    <tr>
      <th>1</th>
      <td>A</td>
      <td>농업, 임업 및 어업(01~03)</td>
      <td>01</td>
      <td>농업</td>
      <td>011</td>
      <td>작물 재배업</td>
      <td>0111</td>
      <td>곡물 및 기타 식량작물 재배업</td>
      <td>01110</td>
      <td>곡물 및 기타 식량작물 재배업</td>
    </tr>
    <tr>
      <th>11</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>012</td>
      <td>축산업</td>
      <td>0121</td>
      <td>소 사육업</td>
      <td>01211</td>
      <td>젖소 사육업</td>
    </tr>
    <tr>
      <th>18</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>013</td>
      <td>작물재배 및 축산 복합농업</td>
      <td>0130</td>
      <td>작물재배 및 축산 복합농업</td>
      <td>01300</td>
      <td>작물재배 및 축산 복합농업</td>
    </tr>
    <tr>
      <th>19</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>014</td>
      <td>작물재배 및 축산 관련 서비스업</td>
      <td>0141</td>
      <td>작물재배 관련 서비스업</td>
      <td>01411</td>
      <td>작물재배 지원 서비스업</td>
    </tr>
    <tr>
      <th>22</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>015</td>
      <td>수렵 및 관련 서비스업</td>
      <td>0150</td>
      <td>수렵 및 관련 서비스업</td>
      <td>01500</td>
      <td>수렵 및 관련 서비스업</td>
    </tr>
    <tr>
      <th>23</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>02</td>
      <td>임업</td>
      <td>020</td>
      <td>임업</td>
      <td>0201</td>
      <td>영림업</td>
      <td>02011</td>
      <td>임업용 종묘 생산업</td>
    </tr>
    <tr>
      <th>28</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>03</td>
      <td>어업</td>
      <td>031</td>
      <td>어로 어업</td>
      <td>0311</td>
      <td>해수면 어업</td>
      <td>03111</td>
      <td>원양 어업</td>
    </tr>
    <tr>
      <th>31</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>032</td>
      <td>양식어업 및 어업관련 서비스업</td>
      <td>0321</td>
      <td>양식 어업</td>
      <td>03211</td>
      <td>해수면 양식 어업</td>
    </tr>
    <tr>
      <th>35</th>
      <td>B</td>
      <td>광업(05~08)</td>
      <td>05</td>
      <td>석탄, 원유 및 천연가스 광업</td>
      <td>051</td>
      <td>석탄 광업</td>
      <td>0510</td>
      <td>석탄 광업</td>
      <td>05100</td>
      <td>석탄 광업</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-c488b6e9-5c18-4599-bd39-cd5701d74f41')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-c488b6e9-5c18-4599-bd39-cd5701d74f41 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-c488b6e9-5c18-4599-bd39-cd5701d74f41');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python
code = code_book[['대분류(21)', '중분류(77)', '소분류(232)', 'Unnamed: 5']][1:].reset_index(drop=True)
```


```python
def na_to_code(data):
  data_l = []
  temp = data[0]

  for i in range(0, len(data)):
    if pd.isna(data[i]):
      data[i] =  temp
    else:
      temp = data[i] 
    data_l.append(temp)
  return data_l
```


```python
big = na_to_code(code.iloc[:,0].tolist())
middle = na_to_code(code.iloc[:,1].tolist())
small = na_to_code(code.iloc[:,2].tolist())
```


```python
code_b = pd.DataFrame(zip(big,middle,small), columns = ['big', 'middle', 'small'])
code_b['y'] = code_b['small'].astype('int64')
code_b['name'] = code['Unnamed: 5']
code_b
code_b.to_excel(path + 'codebook_dict.xlsx', index=False, encoding = 'EUC-KR')
```


```python
code_b = pd.read_excel(path + 'codebook_dict.xlsx', dtype = {'big': str, 'middle': str, 'small': str})
```


```python
code_b = code_b.iloc[:,:-1] # name은 참고용이므로 제외
dict_fin = code_b.set_index('y').T.to_dict('list') # 소분류값을 key 로 한 dictionary
```

## 학습한 모델 불러오기 및 환경 구축


```python
# # train 할 때, 메모리를 많이 사용하여, 비우기
# import gc
# gc.collect()
# del model
# torch.cuda.empty_cache()
```


```python
# test 하기전 모델 기본 값 불러오기

# KoBERT 입력 데이터로 만들기
# BERT 모델에 들어가기 위한 dataset을 만들어주는 클래스
class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,
                 pad, pair):
        transform = nlp.data.BERTSentenceTransform(
            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)

        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i], ))

    def __len__(self):
        return (len(self.labels))

# 토큰화 실행
tokenizer = get_tokenizer()
tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)

# KoBERT 학습모델 만들기
class BERTClassifier(nn.Module):
    def __init__(self,
                 bert,
                 hidden_size = 768,
                 num_classes=225,   ##클래스 수 조정해줘야함##
                 dr_rate=None,
                 params=None):
        super(BERTClassifier, self).__init__()
        self.bert = bert
        self.dr_rate = dr_rate
                 
        self.classifier = nn.Linear(hidden_size , num_classes)
        if dr_rate:
            self.dropout = nn.Dropout(p=dr_rate)
    
    def gen_attention_mask(self, token_ids, valid_length):
        attention_mask = torch.zeros_like(token_ids)
        for i, v in enumerate(valid_length):
            attention_mask[i][:v] = 1
        return attention_mask.float()

    def forward(self, token_ids, valid_length, segment_ids):
        attention_mask = self.gen_attention_mask(token_ids, valid_length)
        
        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))
        if self.dr_rate:
            out = self.dropout(pooler)
        return self.classifier(out)
```

    using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece
    


```python
# import pickle
# # 학습한 model 열기

# with open(path+'model_trial_fin_noclean.pickle', 'rb') as f:
#     model = pickle.load(f)
```


```python
# Setting parameters
# 이건 나중에 최적화 값 찾아봐야할 듯
max_len = 64
batch_size = 128
warmup_ratio = 0.1
num_epochs = 6
max_grad_norm = 1
log_interval = 200
learning_rate = 5e-5 # 0.0001 # 
```


```python
model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)
# Prepare optimizer and schedule (linear warmup and decay)
no_decay = ['bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
]
optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)

checkpoint = torch.load(path + 'final_data/' + 'correct_model_fin.pt') # 학습한 파일 경로 지정
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']
```

    /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
      FutureWarning,
    


```python
model.eval()
```




    BERTClassifier(
      (bert): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(8002, 768, padding_idx=1)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
      (classifier): Linear(in_features=768, out_features=225, bias=True)
      (dropout): Dropout(p=0.5, inplace=False)
    )



## 맞춤법 처리한 제출파일 제출 형식으로 바꾸기


```python
test = pd.read_csv(path + 'final_data/' + 'correct_sub_fin.csv', encoding = 'utf-8-sig')
```


```python
dataset_test = [[str(a), '0'] for a in test['clean_done']]
dataset_test[:20]
```




    [['치킨전문점에서 고객의 주문에 의해 치킨 판매', '0'],
     ['산업공구 다른 소매업자에게 철물 수공구', '0'],
     ['절에서 신도를 대상으로 불교단체 운영', '0'],
     ['영업장에서 고객 요구로 자동차 튜닝', '0'],
     ['실내포장마차에서 접객시설을 갖추고 소주 맥주 제공', '0'],
     ['철 아크릴 코맥스 스크린인쇄 명판', '0'],
     ['음식점 접객시설 가지고 조 개구 이 판매', '0'],
     ['스테인리스를 프레스가공하여 제조 주방용품', '0'],
     ['수리 서비스센터에서 전문수리 수입차', '0'],
     ['약품 화공   미싱 완성품 입고  수선 ', '0'],
     ['밀가루  쇼트닝 원재료 입고  반죽 ', '0'],
     ['이발소에서 일반인 대상으로 고객의 두발을 손질함', '0'],
     ['목재 고객의 요구에 따라 무대장치', '0'],
     ['의원에서 소아 청소년을 대상으로 진료서비스', '0'],
     ['산업공단 조성 시 의뢰받아 사무실에서 측량 토목설계', '0'],
     ['태국 전통 마사지숍 일반 고객 대상 마사지', '0'],
     ['사업장에서 관련 사용자에게 선박엔진부품', '0'],
     ['상가에서 주문에 의해 반찬 도시락', '0'],
     ['문구용품에서 일반인 대상 문구류 사무용품 소매', '0'],
     ['엔지니어링 및 건축 관련 고객 요청에 의해 도면 설계', '0']]




```python
from tqdm.notebook import tqdm
```


```python
# 예측 함수 생성
# Setting parameters
# train 학습 모델 설정할 때와 동일하게 설정
def predict_set(dataset_test):

    test_acc = 0.0

    tokenizer = get_tokenizer()
    tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)

    data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)

    test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=4)

    out_list =[]

    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)

        valid_length= valid_length
        label = label.long().to(device)

        out = model(token_ids, valid_length, segment_ids)
        output = out.detach().cpu().tolist()
        out_list.append(output)

    pd = sum(out_list,[])
    pd_list = pd_list = [np.argmax(i) for i in pd]
    return pd_list
```


```python
p_test = predict_set(dataset_test)
```

    using cached model. /content/drive/MyDrive/nlp_c/eda_nlp/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece
    


      0%|          | 0/782 [00:00<?, ?it/s]



```python
import pickle
# 학습하기전 기존 y값 사전 열기
with open(path+'final_data/' + 's_dictionary', 'rb') as f:
    s_dict = pickle.load(f)
```


```python
test['predict_y'] = p_test
# 1. 모델 학습하기 전 기존 y값 변수로 변환
test['predict_y'] = test['predict_y'].astype('str').map(s_dict) 
cols = ['digit_1', 'digit_2', 'digit_3']
# 2. 코드북에서 소분류를 통해 대/중분류 함께 예측
test[cols] = test['predict_y'].astype('int64').map(dict_fin).apply(lambda x: pd.Series(x))
```


```python
test_fin = test[['AI_id', 'digit_1', 'digit_2', 'digit_3', 'text_obj', 'text_mthd', 'text_deal']]
test_fin.to_csv(path + 'final_data/' + 'submission_fin_0413.csv', index=False, encoding='EUC-KR')
```


```python
test_fin[:50]
```





  <div id="df-ea3534e9-4f75-4ad2-aaea-09d6613cf35c">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AI_id</th>
      <th>digit_1</th>
      <th>digit_2</th>
      <th>digit_3</th>
      <th>text_obj</th>
      <th>text_mthd</th>
      <th>text_deal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>id_000001</td>
      <td>I</td>
      <td>56</td>
      <td>561</td>
      <td>치킨전문점에서</td>
      <td>고객의주문에의해</td>
      <td>치킨판매</td>
    </tr>
    <tr>
      <th>1</th>
      <td>id_000002</td>
      <td>G</td>
      <td>46</td>
      <td>466</td>
      <td>산업공구</td>
      <td>다른 소매업자에게</td>
      <td>철물 수공구</td>
    </tr>
    <tr>
      <th>2</th>
      <td>id_000003</td>
      <td>S</td>
      <td>94</td>
      <td>949</td>
      <td>절에서</td>
      <td>신도을 대상으로</td>
      <td>불교단체운영</td>
    </tr>
    <tr>
      <th>3</th>
      <td>id_000004</td>
      <td>S</td>
      <td>95</td>
      <td>952</td>
      <td>영업장에서</td>
      <td>고객요구로</td>
      <td>자동차튜닝</td>
    </tr>
    <tr>
      <th>4</th>
      <td>id_000005</td>
      <td>I</td>
      <td>56</td>
      <td>562</td>
      <td>실내포장마차에서</td>
      <td>접객시설을 갖추고</td>
      <td>소주,맥주제공</td>
    </tr>
    <tr>
      <th>5</th>
      <td>id_000006</td>
      <td>C</td>
      <td>18</td>
      <td>181</td>
      <td>철,아크릴,포맥스</td>
      <td>스크린인쇄</td>
      <td>명판</td>
    </tr>
    <tr>
      <th>6</th>
      <td>id_000007</td>
      <td>I</td>
      <td>56</td>
      <td>561</td>
      <td>음식점</td>
      <td>접객시설가지고</td>
      <td>조개구이판매</td>
    </tr>
    <tr>
      <th>7</th>
      <td>id_000008</td>
      <td>C</td>
      <td>25</td>
      <td>259</td>
      <td>스테인레스를</td>
      <td>프레스가공하여제조</td>
      <td>주방용품</td>
    </tr>
    <tr>
      <th>8</th>
      <td>id_000009</td>
      <td>S</td>
      <td>95</td>
      <td>952</td>
      <td>수리</td>
      <td>서비스센터에서</td>
      <td>전문수리 수입차</td>
    </tr>
    <tr>
      <th>9</th>
      <td>id_000010</td>
      <td>C</td>
      <td>25</td>
      <td>259</td>
      <td>약품(화공), 미싱</td>
      <td>완성품입고, 수선</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>10</th>
      <td>id_000011</td>
      <td>C</td>
      <td>10</td>
      <td>107</td>
      <td>밀가루, 쇼트닝</td>
      <td>원재료입고, 반죽</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>11</th>
      <td>id_000012</td>
      <td>S</td>
      <td>96</td>
      <td>961</td>
      <td>이발소에서</td>
      <td>일반인 대상으로</td>
      <td>고객의 두발을 손질함</td>
    </tr>
    <tr>
      <th>12</th>
      <td>id_000013</td>
      <td>C</td>
      <td>16</td>
      <td>162</td>
      <td>목재</td>
      <td>고객의 요구에 따라</td>
      <td>무대장치</td>
    </tr>
    <tr>
      <th>13</th>
      <td>id_000014</td>
      <td>Q</td>
      <td>86</td>
      <td>862</td>
      <td>의원에서</td>
      <td>소아,청소년을대상으로</td>
      <td>진료서비스</td>
    </tr>
    <tr>
      <th>14</th>
      <td>id_000015</td>
      <td>M</td>
      <td>72</td>
      <td>729</td>
      <td>산업공단조성시 의뢰받아</td>
      <td>사무실에서</td>
      <td>측량，토목설계</td>
    </tr>
    <tr>
      <th>15</th>
      <td>id_000016</td>
      <td>S</td>
      <td>96</td>
      <td>961</td>
      <td>태국전통마사지샵</td>
      <td>일반고객대상</td>
      <td>마사지</td>
    </tr>
    <tr>
      <th>16</th>
      <td>id_000017</td>
      <td>G</td>
      <td>46</td>
      <td>465</td>
      <td>사업장에서</td>
      <td>관련사용자에게</td>
      <td>선박엔진부품</td>
    </tr>
    <tr>
      <th>17</th>
      <td>id_000018</td>
      <td>C</td>
      <td>10</td>
      <td>107</td>
      <td>상가에서</td>
      <td>주문에 의해</td>
      <td>반찬,도시락</td>
    </tr>
    <tr>
      <th>18</th>
      <td>id_000019</td>
      <td>G</td>
      <td>47</td>
      <td>476</td>
      <td>문구용품에서</td>
      <td>일반인대상</td>
      <td>문구류,사무용품 소매</td>
    </tr>
    <tr>
      <th>19</th>
      <td>id_000020</td>
      <td>M</td>
      <td>72</td>
      <td>721</td>
      <td>엔지니어링및 건축관련</td>
      <td>고객 요청에 의해</td>
      <td>도면설계</td>
    </tr>
    <tr>
      <th>20</th>
      <td>id_000021</td>
      <td>C</td>
      <td>25</td>
      <td>259</td>
      <td>스테인레스메탈</td>
      <td>도금</td>
      <td>샤워해드제조</td>
    </tr>
    <tr>
      <th>21</th>
      <td>id_000022</td>
      <td>G</td>
      <td>46</td>
      <td>464</td>
      <td>매니큐어용기</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>22</th>
      <td>id_000023</td>
      <td>Q</td>
      <td>87</td>
      <td>872</td>
      <td>요양보호센터에서</td>
      <td>고객에 의뢰를 받아</td>
      <td>요양보호사파견</td>
    </tr>
    <tr>
      <th>23</th>
      <td>id_000024</td>
      <td>S</td>
      <td>96</td>
      <td>969</td>
      <td>점포</td>
      <td>일반인</td>
      <td>세탁서비스</td>
    </tr>
    <tr>
      <th>24</th>
      <td>id_000025</td>
      <td>C</td>
      <td>32</td>
      <td>320</td>
      <td>우드카운티에서</td>
      <td>나무를이용해</td>
      <td>가구주문제작</td>
    </tr>
    <tr>
      <th>25</th>
      <td>id_000026</td>
      <td>S</td>
      <td>95</td>
      <td>951</td>
      <td>영업장에서</td>
      <td>고객요구로</td>
      <td>컴퓨터.핸드폰수리</td>
    </tr>
    <tr>
      <th>26</th>
      <td>id_000027</td>
      <td>G</td>
      <td>47</td>
      <td>474</td>
      <td>구두가게에서</td>
      <td>일반인을 대상으로</td>
      <td>구두소매</td>
    </tr>
    <tr>
      <th>27</th>
      <td>id_000028</td>
      <td>L</td>
      <td>68</td>
      <td>682</td>
      <td>사업장에서</td>
      <td>고객의뢰로</td>
      <td>중개서비스</td>
    </tr>
    <tr>
      <th>28</th>
      <td>id_000029</td>
      <td>L</td>
      <td>68</td>
      <td>682</td>
      <td>부동산</td>
      <td>계약및중개수수료받고</td>
      <td>부동산거래중개서비스</td>
    </tr>
    <tr>
      <th>29</th>
      <td>id_000030</td>
      <td>R</td>
      <td>91</td>
      <td>912</td>
      <td>pc방에서</td>
      <td>인터넷시설을 갖추고</td>
      <td>인터넷서비스제공</td>
    </tr>
    <tr>
      <th>30</th>
      <td>id_000031</td>
      <td>P</td>
      <td>85</td>
      <td>855</td>
      <td>사업장에서</td>
      <td>어린이들을 대상으로</td>
      <td>방문교육학습지</td>
    </tr>
    <tr>
      <th>31</th>
      <td>id_000032</td>
      <td>G</td>
      <td>46</td>
      <td>463</td>
      <td>산업사용자에게</td>
      <td>선별장에서</td>
      <td>농산물도매</td>
    </tr>
    <tr>
      <th>32</th>
      <td>id_000033</td>
      <td>I</td>
      <td>56</td>
      <td>561</td>
      <td>음식점에서</td>
      <td>접객시설을갖추고</td>
      <td>보쌈음식</td>
    </tr>
    <tr>
      <th>33</th>
      <td>id_000034</td>
      <td>C</td>
      <td>25</td>
      <td>251</td>
      <td>샤시,방범창</td>
      <td>사업장에서</td>
      <td>제작.용접</td>
    </tr>
    <tr>
      <th>34</th>
      <td>id_000035</td>
      <td>S</td>
      <td>96</td>
      <td>961</td>
      <td>이용원</td>
      <td>남성두발서비스</td>
      <td>커트,파마</td>
    </tr>
    <tr>
      <th>35</th>
      <td>id_000036</td>
      <td>I</td>
      <td>56</td>
      <td>561</td>
      <td>음식점에서</td>
      <td>접객시설을갗추고</td>
      <td>돼지수육,순대정식</td>
    </tr>
    <tr>
      <th>36</th>
      <td>id_000037</td>
      <td>G</td>
      <td>47</td>
      <td>478</td>
      <td>꽃시장에서</td>
      <td>일반소비자 대상으로</td>
      <td>관엽 소매</td>
    </tr>
    <tr>
      <th>37</th>
      <td>id_000038</td>
      <td>J</td>
      <td>61</td>
      <td>612</td>
      <td>카드사등</td>
      <td>G/W 기술</td>
      <td>SMS 서비스</td>
    </tr>
    <tr>
      <th>38</th>
      <td>id_000039</td>
      <td>C</td>
      <td>13</td>
      <td>139</td>
      <td>PP.PET</td>
      <td>원료투입, 압출</td>
      <td>원사</td>
    </tr>
    <tr>
      <th>39</th>
      <td>id_000040</td>
      <td>Q</td>
      <td>86</td>
      <td>862</td>
      <td>내과의원에서</td>
      <td>외래환자위주로</td>
      <td>내과</td>
    </tr>
    <tr>
      <th>40</th>
      <td>id_000041</td>
      <td>G</td>
      <td>47</td>
      <td>478</td>
      <td>안경접에서</td>
      <td>일반고객을 대상으로</td>
      <td>안경,렌즈</td>
    </tr>
    <tr>
      <th>41</th>
      <td>id_000042</td>
      <td>C</td>
      <td>18</td>
      <td>181</td>
      <td>종이인쇄물</td>
      <td>재단기사용</td>
      <td>재단(인쇄물)</td>
    </tr>
    <tr>
      <th>42</th>
      <td>id_000043</td>
      <td>G</td>
      <td>47</td>
      <td>477</td>
      <td>가정용가스연료를</td>
      <td>가정소비자에게 소매</td>
      <td>충전된 가정용LPG70</td>
    </tr>
    <tr>
      <th>43</th>
      <td>id_000044</td>
      <td>I</td>
      <td>56</td>
      <td>562</td>
      <td>다방에서</td>
      <td>일반인을 대상으로</td>
      <td>커피</td>
    </tr>
    <tr>
      <th>44</th>
      <td>id_000045</td>
      <td>C</td>
      <td>33</td>
      <td>339</td>
      <td>제조업</td>
      <td>업자를 대상으로</td>
      <td>칫솔,부러쉬</td>
    </tr>
    <tr>
      <th>45</th>
      <td>id_000046</td>
      <td>H</td>
      <td>52</td>
      <td>521</td>
      <td>사업장에서</td>
      <td>철파이프보관</td>
      <td>창고(물류)</td>
    </tr>
    <tr>
      <th>46</th>
      <td>id_000047</td>
      <td>I</td>
      <td>55</td>
      <td>551</td>
      <td>펜션에서</td>
      <td>관광객을 대상으로</td>
      <td>숙박서비스제공</td>
    </tr>
    <tr>
      <th>47</th>
      <td>id_000048</td>
      <td>G</td>
      <td>46</td>
      <td>464</td>
      <td>소매업자에게대량으로도매</td>
      <td>판매장에서</td>
      <td>비누,세정제,화장품,치약</td>
    </tr>
    <tr>
      <th>48</th>
      <td>id_000049</td>
      <td>M</td>
      <td>73</td>
      <td>732</td>
      <td>주문에 의해</td>
      <td>인테리어</td>
      <td>디자인</td>
    </tr>
    <tr>
      <th>49</th>
      <td>id_000050</td>
      <td>G</td>
      <td>47</td>
      <td>471</td>
      <td>매장에서</td>
      <td>일반소비자에게</td>
      <td>식료품소매1</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-ea3534e9-4f75-4ad2-aaea-09d6613cf35c')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-ea3534e9-4f75-4ad2-aaea-09d6613cf35c button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-ea3534e9-4f75-4ad2-aaea-09d6613cf35c');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python

```
